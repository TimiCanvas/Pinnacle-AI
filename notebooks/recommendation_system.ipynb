{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9c2dd9",
   "metadata": {},
   "source": [
    "# Hybrid Banking Recommendation System: ALS + LLM\n",
    "## Zenith Bank Product Recommendation Engine\n",
    "\n",
    "This notebook implements a hybrid recommendation system that combines:\n",
    "1. **Collaborative Filtering (ALS)** - For data-driven product recommendations\n",
    "2. **LLM (GPT-4)** - For context awareness, explanations, and personalization\n",
    "\n",
    "**Architecture:**\n",
    "- ALS generates base recommendations from transaction patterns\n",
    "- LLM enhances with context, explanations, and customer segmentation\n",
    "- Feature engineering creates rich customer profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1101297",
   "metadata": {},
   "source": [
    "## Cell 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4621e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "#!pip install -q openai python-dotenv implicit scikit-learn scipy pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441c427",
   "metadata": {},
   "source": [
    "## Cell 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58656198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, ndcg_score\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import time\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import json\n",
    "from decimal import Decimal\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e4fbe4",
   "metadata": {},
   "source": [
    "## Cell 3: Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6933a732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Configuration Loaded from .env:\n",
      "{\n",
      "  \"als\": {\n",
      "    \"factors\": 50,\n",
      "    \"regularization\": 0.01,\n",
      "    \"iterations\": 15,\n",
      "    \"alpha\": 40.0\n",
      "  },\n",
      "  \"llm\": {\n",
      "    \"model\": \"gpt-4o-mini\",\n",
      "    \"temperature\": 0.3,\n",
      "    \"max_tokens\": 2000\n",
      "  },\n",
      "  \"recommendation\": {\n",
      "    \"top_n\": 5,\n",
      "    \"final_n\": 3\n",
      "  },\n",
      "  \"feature_engineering\": {\n",
      "    \"recency_days\": 90,\n",
      "    \"min_transactions\": 3\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# API KEYS AND CONFIGURATION (loaded from .env)\n",
    "# ============================================================================\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "CONFIG = {\n",
    "    'als': {\n",
    "        'factors': int(os.getenv('ALS_FACTORS', 50)),\n",
    "        'regularization': float(os.getenv('ALS_REGULARIZATION', 0.01)),\n",
    "        'iterations': int(os.getenv('ALS_ITERATIONS', 15)),\n",
    "        'alpha': float(os.getenv('ALS_ALPHA', 40.0))\n",
    "    },\n",
    "    'llm': {\n",
    "        'model': os.getenv('LLM_MODEL', 'gpt-4o-mini'),\n",
    "        'temperature': float(os.getenv('LLM_TEMPERATURE', 0.3)),\n",
    "        'max_tokens': int(os.getenv('LLM_MAX_TOKENS', 2000))\n",
    "    },\n",
    "    'recommendation': {\n",
    "        'top_n': int(os.getenv('RECOMMENDATION_TOP_N', 5)),\n",
    "        'final_n': int(os.getenv('RECOMMENDATION_FINAL_N', 3))\n",
    "    },\n",
    "    'feature_engineering': {\n",
    "        'recency_days': int(os.getenv('FEATURE_RECENCY_DAYS', 90)),\n",
    "        'min_transactions': int(os.getenv('FEATURE_MIN_TRANSACTIONS', 3))\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Configuration Loaded from .env:\")\n",
    "print(json.dumps(CONFIG, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585a32a",
   "metadata": {},
   "source": [
    "## Cell 4: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca850beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session initialized successfully\n",
      "üìÇ Loading datasets...\n",
      "\n",
      "‚úÖ Loaded 1,200,000 transactions\n",
      "‚úÖ Loaded 42 products\n",
      "‚úÖ Loaded 3,164 conversations\n",
      "‚úÖ Loaded 100,000 customers\n",
      "\n",
      "üìä Schema Preview:\n",
      "root\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Trans_Amount: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      " |-- Deb_or_credit: string (nullable = true)\n",
      " |-- Narration: string (nullable = true)\n",
      " |-- Tran_Id: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Account_Type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PinnacleAI_DataLoad\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark session initialized successfully\")\n",
    "\n",
    "# File paths\n",
    "TRANSACTIONS_TABLE = r'C:\\Users\\adeye\\Documents\\Pinnacle-AI\\dataset\\transaction.csv'\n",
    "PRODUCTS_TABLE = r'C:\\Users\\adeye\\Documents\\Pinnacle-AI\\dataset\\product.csv'\n",
    "CONVERSATIONS_TABLE = r'C:\\Users\\adeye\\Documents\\Pinnacle-AI\\dataset\\interaction_pd.csv'\n",
    "CUSTOMERS_TABLE = r'C:\\Users\\adeye\\Documents\\Pinnacle-AI\\dataset\\customers.csv'\n",
    "\n",
    "print(\"üìÇ Loading datasets...\\n\")\n",
    "\n",
    "try:\n",
    "    df_transactions = spark.read.option(\"header\", True).csv(TRANSACTIONS_TABLE)\n",
    "    df_products = spark.read.option(\"header\", True).csv(PRODUCTS_TABLE)\n",
    "    df_conversations = spark.read.option(\"header\", True).csv(CONVERSATIONS_TABLE)\n",
    "    df_customers = spark.read.option(\"header\", True).csv(CUSTOMERS_TABLE)\n",
    "\n",
    "    print(f\"‚úÖ Loaded {df_transactions.count():,} transactions\")\n",
    "    print(f\"‚úÖ Loaded {df_products.count():,} products\")\n",
    "    print(f\"‚úÖ Loaded {df_conversations.count():,} conversations\")\n",
    "    print(f\"‚úÖ Loaded {df_customers.count():,} customers\")\n",
    "\n",
    "    print(\"\\nüìä Schema Preview:\")\n",
    "    df_transactions.printSchema()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad4387",
   "metadata": {},
   "source": [
    "## Cell 5: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2381f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRANSACTIONS SAMPLE:\n",
      "+-----------+------------+----------+---------------------+-------------+--------------------+-----------+---------+----------------------------+\n",
      "|Customer_ID|Trans_Amount|Date      |Destination          |Deb_or_credit|Narration           |Tran_Id    |Category |Account_Type                |\n",
      "+-----------+------------+----------+---------------------+-------------+--------------------+-----------+---------+----------------------------+\n",
      "|ZB060544   |2161947.29  |2023-01-01|Maintenance Services |D            |service charge      |TR000054680|Housing  |Timeless Account Savings    |\n",
      "|ZB032114   |11331.88    |2023-01-01|Mobil Filling Station|D            |premium motor spirit|TR000874920|Transport|Gold Premium Current Account|\n",
      "|ZB096997   |497269.45   |2023-01-01|JAMB Registration    |D            |school fees payment |TR000315856|Education|Timeless Account Current    |\n",
      "|ZB011060   |382993.75   |2023-01-01|Slot Limited         |D            |fashion items       |TR000827345|Shopping |Timeless Account Savings    |\n",
      "|ZB020249   |1706431.91  |2023-01-01|Mobile Transfer      |C            |bill settlement     |TR000010619|Transfer |Gold Premium Current Account|\n",
      "+-----------+------------+----------+---------------------+-------------+--------------------+-----------+---------+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      " PRODUCTS SAMPLE:\n",
      "+----------+---------------------------------+-----------------------+-----------+---------------+---------------+---------------+-------------+-------------------------+-----------+-----------------------+---------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+---------------------------------------------+------------------------------------------------------------+----------------------------------------------------------------------+------+\n",
      "|Product_ID|Product_Name                     |Product_Category       |Age_Range  |Opening_Balance|Minimum_Balance|Maximum_Balance|Currency     |Interest_Rate            |Monthly_Fee|Account_Maintenance_Fee|Key_Features                                                                                                               |Digital_Channels                                                |Card_Type                                    |Target_Audience                                             |Special_Benefits                                                      |Status|\n",
      "+----------+---------------------------------+-----------------------+-----------+---------------+---------------+---------------+-------------+-------------------------+-----------+-----------------------+---------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+---------------------------------------------+------------------------------------------------------------+----------------------------------------------------------------------+------+\n",
      "|PROD001   |Zenith Childrens Account (ZECA)  |Savings Account        |0-15 years |0              |0              |No limit       |Naira, Dollar|Competitive              |0          |0                      |Zero opening balance, Scholarship opportunities, Education loan, Customized prepaid card, Annual Children Parade invitation|Zenith Mobile App, *966# EazyBanking, ZIVA                      |Customized Prepaid Card (upon parent request)|Children aged 0-15 years                                    |Teaches children to save, Parents can save for child future           |Active|\n",
      "|PROD002   |Aspire Account                   |Student Savings Account|16-25 years|0              |0              |‚Ç¶25,000,000    |Naira        |Competitive              |0          |0                      |Zero opening balance, Zero minimum balance, Customized debit card, Discounted card fee ‚Ç¶500                                |Zenith Mobile App, *966# EazyBanking, ZIVA                      |Customized Debit Card                        |Nigerian undergraduate students aged 16-25                  |Enhances student lifestyle, Supports aspirations and dreams           |Active|\n",
      "|PROD003   |Aspire Lite                      |Student Savings Account|16-25 years|0              |0              |‚Ç¶300,000       |Naira        |Competitive              |0          |0                      |Less documentation, ‚Ç¶50,000 max single deposit, ‚Ç¶20,000 max daily transfer, Aspire Verve debit card                        |Zenith Mobile App, *966# EazyBanking, ZIVA                      |Aspire Debit Card (Verve)                    |Students with limited documentation                         |Easier account opening, Transaction limits for safety                 |Active|\n",
      "|PROD004   |Ethical Savings Account          |Savings Account        |All ages   |0              |0              |No limit       |Naira        |0% (Non-interest bearing)|0          |0                      |Virtual account linked to parent account, No interest, 90-day withdrawal restriction, No minimum deposit                   |Free Email/SMS alerts                                           |N/A                                          |Customers wanting zero-interest savings for specific reasons|Ethical banking option, Helps save for specific goals without interest|Active|\n",
      "|PROD005   |Zenith Individual Current Account|Current Account        |18+ years  |0              |0              |No limit       |Naira        |0%                       |Standard   |Standard               |Zero opening balance, Dividend/draft lodgment, Email/SMS alerts (AlertZ), Free digital banking access                      |Zenith Internet Banking, Mobile Banking, *966# EazyBanking, ZIVA|Zenith Debit and Credit Cards                |Individual customers needing flexible banking               |Flexibility to bank anywhere in the world                             |Active|\n",
      "+----------+---------------------------------+-----------------------+-----------+---------------+---------------+---------------+-------------+-------------------------+-----------+-----------------------+---------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+---------------------------------------------+------------------------------------------------------------+----------------------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      " CUSTOMERS SAMPLE:\n",
      "+-----------+--------------------------+-----------+---------+------+-------------+---+------------+---------------------------------+--------------------------------+----------+-----------+----------+-----------------+-----------------------+--------------+---------------+--------------------------+--------------------+-----------+--------+\n",
      "|Customer_ID|Full_Name                 |First_Name |Last_Name|Gender|Date_of_Birth|Age|Phone_Number|Email                            |Address                         |City      |State      |Occupation|Employment_Status|Income_Bracket         |Marital_Status|Education_Level|Account_Type              |Account_Opening_Date|BVN        |Status  |\n",
      "+-----------+--------------------------+-----------+---------+------+-------------+---+------------+---------------------------------+--------------------------------+----------+-----------+----------+-----------------+-----------------------+--------------+---------------+--------------------------+--------------------+-----------+--------+\n",
      "|ZB000001   |Engr. Chukwuemeka Abubakar|Chukwuemeka|Abubakar |Male  |1993-09-08   |32 |08023756669 |chukwuemeka.abubakar693@gmail.com|9 Allen Avenue, Old Town        |Old Town  |Kwara      |Teacher   |Self-Employed    |‚Ç¶100,000 - ‚Ç¶250,000    |Married       |PhD            |Individual Current Account|2022-09-18          |68175080694|Inactive|\n",
      "|ZB000002   |Miss Chioma Brown         |Chioma     |Brown    |Female|1980-05-31   |45 |08047295260 |chioma.brown160@yahoo.com        |27 Awolowo Road, New Layout     |New Layout|Sokoto     |Consultant|Employed         |‚Ç¶1,000,000 - ‚Ç¶2,500,000|Single        |M.Sc           |Individual Current Account|2022-06-23          |84987658854|Active  |\n",
      "|ZB000003   |Dr. Ruth Uzor             |Ruth       |Uzor     |Female|1994-11-23   |30 |08019335534 |ruth.uzor47@yahoo.com            |21 Ahmadu Bello Way, New Layout |New Layout|Sokoto     |Engineer  |Retired          |‚Ç¶1,000,000 - ‚Ç¶2,500,000|Married       |M.Sc           |Individual Current Account|2022-04-19          |60827117278|Active  |\n",
      "|ZB000004   |Engr. David Brown         |David      |Brown    |Male  |1963-12-30   |61 |08019583482 |david.brown624@yahoo.com         |42 Circular Road, GRA           |GRA       |Cross River|Consultant|Self-Employed    |> ‚Ç¶2,500,000           |Single        |M.Sc           |Timeless Account Current  |2022-01-08          |40305022733|Active  |\n",
      "|ZB000005   |Miss Ngozi Uzor           |Ngozi      |Uzor     |Female|1951-06-21   |74 |08038538251 |ngozi.uzor672@hotmail.com        |37 Independence Avenue, Old Town|Old Town  |Akwa Ibom  |Consultant|Self-Employed    |> ‚Ç¶2,500,000           |Divorced      |M.Sc           |Timeless Account Savings  |2021-09-03          |68345352569|Active  |\n",
      "+-----------+--------------------------+-----------+---------+------+-------------+---+------------+---------------------------------+--------------------------------+----------+-----------+----------+-----------------+-----------------------+--------------+---------------+--------------------------+--------------------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      " CONVERSATIONS SAMPLE:\n",
      "+-----------+---------------------------------------+------------------+\n",
      "|Customer_ID|Product_Name                           |interaction_score |\n",
      "+-----------+---------------------------------------+------------------+\n",
      "|ZB019348   |Ethical Savings Account                |1.8000000000000003|\n",
      "|ZB019479   |Timeless Savings Account               |1.5000000000000002|\n",
      "|ZB033831   |Zenith Platinum Premium Current Account|2.7               |\n",
      "|ZB034908   |Zenith Salary Savings Account          |1.5000000000000002|\n",
      "|ZB040460   |Salary Advance                         |1.5000000000000002|\n",
      "+-----------+---------------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      " TRANSACTION STATISTICS:\n",
      "+-------+-----------+------------------+----------+----------------+-------------+-------------------------+-----------+---------+------------------------+\n",
      "|summary|Customer_ID|Trans_Amount      |Date      |Destination     |Deb_or_credit|Narration                |Tran_Id    |Category |Account_Type            |\n",
      "+-------+-----------+------------------+----------+----------------+-------------+-------------------------+-----------+---------+------------------------+\n",
      "|count  |1200000    |1200000           |1200000   |1200000         |1200000      |1200000                  |1200000    |1200000  |1200000                 |\n",
      "|mean   |null       |277695.12331866444|null      |null            |null         |null                     |null       |null     |null                    |\n",
      "|stddev |null       |729101.2532123554 |null      |null            |null         |null                     |null       |null     |null                    |\n",
      "|min    |ZB000001   |1000.04           |2023-01-01|9mobile Recharge|C            |DSTV subscription renewal|TR000000001|Dining   |Aspire Account          |\n",
      "|max    |ZB100000   |99999.43          |2024-12-31|Zenith Transfer |D            |weekly shopping          |TR001200000|Utilities|Timeless Account Savings|\n",
      "+-------+-----------+------------------+----------+----------------+-------------+-------------------------+-----------+---------+------------------------+\n",
      "\n",
      "\n",
      " Missing Values Per Column:\n",
      "+-----------+------------+----+-----------+-------------+---------+-------+--------+------------+\n",
      "|Customer_ID|Trans_Amount|Date|Destination|Deb_or_credit|Narration|Tran_Id|Category|Account_Type|\n",
      "+-----------+------------+----+-----------+-------------+---------+-------+--------+------------+\n",
      "|0          |0           |0   |0          |0            |0        |0      |0       |0           |\n",
      "+-----------+------------+----+-----------+-------------+---------+-------+--------+------------+\n",
      "\n",
      "\n",
      " DATA OVERVIEW:\n",
      "Total Rows: 1,200,000\n",
      "Unique Customer_ID: 99,998\n",
      "Unique Trans_Amount: 1,131,985\n",
      "Unique Date: 731\n",
      "Unique Destination: 79\n",
      "Unique Deb_or_credit: 2\n",
      "Unique Narration: 83\n",
      "Unique Tran_Id: 1,200,000\n",
      "Unique Category: 11\n",
      "Unique Account_Type: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n TRANSACTIONS SAMPLE:\")\n",
    "df_transactions.show(5, truncate=False)\n",
    "\n",
    "print(\"\\n PRODUCTS SAMPLE:\")\n",
    "df_products.show(5, truncate=False)\n",
    "\n",
    "print(\"\\n CUSTOMERS SAMPLE:\")\n",
    "df_customers.show(5, truncate=False)\n",
    "\n",
    "print(\"\\n CONVERSATIONS SAMPLE:\")\n",
    "df_conversations.show(5, truncate=False)\n",
    "\n",
    "print(\"\\n TRANSACTION STATISTICS:\")\n",
    "df_transactions.describe().show(truncate=False)\n",
    "\n",
    "print(\"\\n Missing Values Per Column:\")\n",
    "missing_df = df_transactions.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c)\n",
    "    for c in df_transactions.columns\n",
    "])\n",
    "missing_df.show(truncate=False)\n",
    "\n",
    "print(\"\\n DATA OVERVIEW:\")\n",
    "print(f\"Total Rows: {df_transactions.count():,}\")\n",
    "for col in df_transactions.columns:\n",
    "    unique_count = df_transactions.select(col).distinct().count()\n",
    "    print(f\"Unique {col}: {unique_count:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9933a",
   "metadata": {},
   "source": [
    "## Cell 6: Feature Engineering - Create Interaction Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1391e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Loaded:\n",
      "{\n",
      "  \"als\": {\n",
      "    \"factors\": 50,\n",
      "    \"regularization\": 0.01,\n",
      "    \"iterations\": 15,\n",
      "    \"alpha\": 40.0\n",
      "  },\n",
      "  \"llm\": {\n",
      "    \"model\": \"gpt-4o-mini\",\n",
      "    \"temperature\": 0.3,\n",
      "    \"max_tokens\": 2000\n",
      "  },\n",
      "  \"recommendation\": {\n",
      "    \"top_n\": 5,\n",
      "    \"final_n\": 3\n",
      "  },\n",
      "  \"feature_engineering\": {\n",
      "    \"recency_days\": 90,\n",
      "    \"min_transactions\": 3\n",
      "  }\n",
      "}\n",
      "\n",
      "Creating interaction matrix...\n",
      "\n",
      "Loaded 42 products\n",
      "Found current products for 92202 customers\n",
      "Sampled 10 customers from 100000\n",
      "Batch 1: 5 matches\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17608\\1903319787.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOPENAI_API_KEY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m interaction_df = create_customer_product_interactions(\n\u001b[0m\u001b[0;32m    236\u001b[0m     \u001b[0mdf_customers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0mdf_products\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mopenai_client\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17608\\1903319787.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(df_custs, df_products, openai_client, model_name, customer_sample_size, batch_size, temperature, top_n_products, rate_limit_delay, additional_rules, test_mode)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfallback_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Customer_ID\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Product_Name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"interaction_score\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_interactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m     \u001b[0minteraction_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;31m# Remove current products\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustomer_current_products\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adeye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m             \u001b[0mhas_pandas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_pandas\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[1;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m             return super(SparkSession, self).createDataFrame(\n\u001b[0m\u001b[0;32m    674\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adeye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    335\u001b[0m                         \u001b[1;34m\"fallback with 'spark.sql.execution.arrow.pyspark.fallback.enabled' \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                         \u001b[1;34m\"has been set to false.\\n  %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m                     \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                     \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adeye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, pdf, schema, timezone)\u001b[0m\n\u001b[0;32m    367\u001b[0m                                 \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                                 \u001b[0mcopied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m                             \u001b[0mpdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_series_convert_timestamps_tz_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcopied\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\adeye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6314\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6315\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6316\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6317\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PINNACLE AI - INTERACTION MATRIX CREATION (OPENAI + PYSPARK, PYTHON 3.13 SAFE)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# ENVIRONMENT AND CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY not found in environment. Please set it in .env\")\n",
    "\n",
    "CONFIG = {\n",
    "    'als': {\n",
    "        'factors': 50,\n",
    "        'regularization': 0.01,\n",
    "        'iterations': 15,\n",
    "        'alpha': 40.0\n",
    "    },\n",
    "    'llm': {\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': 0.3,\n",
    "        'max_tokens': 2000\n",
    "    },\n",
    "    'recommendation': {\n",
    "        'top_n': 5,\n",
    "        'final_n': 3\n",
    "    },\n",
    "    'feature_engineering': {\n",
    "        'recency_days': 90,\n",
    "        'min_transactions': 3\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration Loaded:\")\n",
    "print(json.dumps(CONFIG, indent=2))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# INITIALIZE SPARK SESSION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PinnacleAI_InteractionMatrix\").getOrCreate()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# FUNCTION: CREATE CUSTOMER-PRODUCT INTERACTIONS\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def create_customer_product_interactions(\n",
    "    df_custs,\n",
    "    df_products,\n",
    "    openai_client=None,\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    customer_sample_size=None,\n",
    "    batch_size=30,\n",
    "    temperature=0.3,\n",
    "    top_n_products=None,\n",
    "    rate_limit_delay=0.5,\n",
    "    additional_rules=None,\n",
    "    test_mode=False\n",
    "):\n",
    "    start_time = time.time()\n",
    "    print(\"\\nCreating full interaction matrix (dense scoring)...\\n\")\n",
    "\n",
    "    # Prepare product data\n",
    "    all_product_names = [r[\"Product_Name\"] for r in df_products.select(\"Product_Name\").collect()]\n",
    "    print(f\"Loaded {len(all_product_names)} products\")\n",
    "\n",
    "    # Keyword extraction\n",
    "    df_custs_kw = df_custs.withColumn(\n",
    "        \"account_keywords\",\n",
    "        F.regexp_extract(F.lower(F.col(\"Account_Type\")),\n",
    "                         r\"(current|savings|sme|student|premium|platinum|children|aspire)\", 1)\n",
    "    )\n",
    "    df_prod_kw = df_products.withColumn(\n",
    "        \"product_keywords\",\n",
    "        F.regexp_extract(F.lower(F.col(\"Product_Name\")),\n",
    "                         r\"(current|savings|sme|student|premium|platinum|children|aspire)\", 1)\n",
    "    )\n",
    "\n",
    "    keyword_map = df_prod_kw.filter(F.col(\"product_keywords\") != \"\").select(\n",
    "        \"product_keywords\", \"Product_Name\"\n",
    "    ).distinct()\n",
    "\n",
    "    df_cust_current = df_custs_kw.join(\n",
    "        keyword_map,\n",
    "        df_custs_kw.account_keywords == keyword_map.product_keywords,\n",
    "        \"left\"\n",
    "    ).withColumnRenamed(\"Product_Name\", \"current_product\")\n",
    "\n",
    "    customer_current_products_df = df_cust_current.filter(\n",
    "        F.col(\"current_product\").isNotNull()\n",
    "    ).groupBy(\"Customer_ID\").agg(F.collect_set(\"current_product\").alias(\"current_products\"))\n",
    "\n",
    "    print(f\"Found current products for {customer_current_products_df.count()} customers\")\n",
    "\n",
    "    customer_current_products = {\n",
    "        r[\"Customer_ID\"]: set(r[\"current_products\"])\n",
    "        for r in customer_current_products_df.collect()\n",
    "    }\n",
    "\n",
    "    # Sampling\n",
    "    total_customers = df_custs.count()\n",
    "    if customer_sample_size and total_customers > customer_sample_size:\n",
    "        df_custs = df_custs.sample(fraction=customer_sample_size / total_customers, seed=42).limit(customer_sample_size)\n",
    "        print(f\"Sampled {customer_sample_size} customers from {total_customers}\")\n",
    "\n",
    "    # Base rules\n",
    "    general_rules = [\n",
    "        \"- Match products to customer demographics (age, income, occupation)\",\n",
    "        \"- Exclude products already owned by the customer\",\n",
    "        \"- Respect target audience and minimum balance criteria\",\n",
    "        \"- Score every product for every customer, even if not relevant\"\n",
    "    ]\n",
    "    if additional_rules:\n",
    "        general_rules += [f\"- {r}\" for r in additional_rules]\n",
    "    rules_text = \"\\n\".join(general_rules)\n",
    "\n",
    "    cust_rows = df_custs.collect()\n",
    "    all_interactions = []\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # Backoff-enabled API call helper\n",
    "    # --------------------------------------------------------------------\n",
    "    @backoff.on_exception(backoff.expo, Exception, max_tries=3, jitter=None)\n",
    "    def call_openai(prompt):\n",
    "        return openai_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a precise JSON generator.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # TEST MODE\n",
    "    # --------------------------------------------------------------------\n",
    "    if test_mode:\n",
    "        print(\"Running in TEST MODE (no OpenAI calls)...\")\n",
    "        for cust in cust_rows:\n",
    "            for prod in all_product_names:\n",
    "                score = float(round(5 + 5 * (hash(cust[\"Customer_ID\"] + prod) % 100) / 100, 2))\n",
    "                all_interactions.append({\n",
    "                    \"Customer_ID\": cust[\"Customer_ID\"],\n",
    "                    \"Product_Name\": prod,\n",
    "                    \"interaction_score\": score\n",
    "                })\n",
    "    else:\n",
    "        if not openai_client:\n",
    "            raise ValueError(\"OpenAI client must be initialized when test_mode=False\")\n",
    "\n",
    "        for i in range(0, len(cust_rows), batch_size):\n",
    "            batch = cust_rows[i:i + batch_size]\n",
    "            summaries = []\n",
    "            for cust in batch:\n",
    "                details = [f\"Customer {cust['Customer_ID']}\"]\n",
    "                for col in df_custs.columns:\n",
    "                    if col != \"Customer_ID\":\n",
    "                        val = cust[col]\n",
    "                        if val:\n",
    "                            details.append(f\"- {col}: {val}\")\n",
    "                summaries.append(\"\\n\".join(details))\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            You are a banking recommendation AI. \n",
    "            For every customer, assign a product fit score between 0 and 10 for each product ‚Äî even if the fit is low.\n",
    "\n",
    "            CUSTOMERS:\n",
    "            {chr(10).join(summaries)}\n",
    "\n",
    "            PRODUCTS:\n",
    "            {'; '.join(all_product_names)}\n",
    "\n",
    "            RULES:\n",
    "            {rules_text}\n",
    "\n",
    "            Return only JSON:\n",
    "            {{\n",
    "                \"matches\": [\n",
    "                    {{\"customer_id\": \"C001\", \"product\": \"Product Name\", \"score\": 8.5}},\n",
    "                    ...\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                response = call_openai(prompt)\n",
    "                content = response.choices[0].message.content\n",
    "                result = json.loads(content)\n",
    "\n",
    "                if \"matches\" not in result:\n",
    "                    print(f\"‚ö†Ô∏è No matches returned in batch {i//batch_size + 1}\")\n",
    "                    continue\n",
    "\n",
    "                for match in result[\"matches\"]:\n",
    "                    all_interactions.append({\n",
    "                        \"Customer_ID\": match[\"customer_id\"],\n",
    "                        \"Product_Name\": match[\"product\"],\n",
    "                        \"interaction_score\": float(match[\"score\"])\n",
    "                    })\n",
    "                print(f\"‚úÖ Batch {i//batch_size + 1}: {len(result['matches'])} scores generated\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error in batch {i//batch_size + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "            time.sleep(rate_limit_delay)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # Convert to Spark DataFrame (Python 3.13 safe)\n",
    "    # --------------------------------------------------------------------\n",
    "    if not all_interactions:\n",
    "        print(\"‚ö†Ô∏è No interactions generated; using fallback values.\")\n",
    "        fallback_data = [(cust[\"Customer_ID\"], all_product_names[0], 5.0) for cust in cust_rows]\n",
    "        pdf = pd.DataFrame(fallback_data, columns=[\"Customer_ID\", \"Product_Name\", \"interaction_score\"])\n",
    "    else:\n",
    "        pdf = pd.DataFrame(all_interactions)\n",
    "\n",
    "    interaction_df = spark.createDataFrame(pdf)\n",
    "\n",
    "    # Remove already-owned products\n",
    "    if customer_current_products:\n",
    "        current_pairs = [(cust, prod) for cust, prods in customer_current_products.items() for prod in prods]\n",
    "        current_df = spark.createDataFrame(current_pairs, [\"Customer_ID\", \"Product_Name\"])\n",
    "        interaction_df = interaction_df.join(current_df, [\"Customer_ID\", \"Product_Name\"], \"left_anti\")\n",
    "\n",
    "    # Consolidate max score per pair\n",
    "    interaction_df = interaction_df.groupBy(\"Customer_ID\", \"Product_Name\").agg(\n",
    "        F.max(\"interaction_score\").alias(\"interaction_score\")\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Completed in {elapsed:.2f}s with {interaction_df.count()} interactions.\")\n",
    "\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    interaction_df.write.mode(\"overwrite\").parquet(\"data/interactions.parquet\")\n",
    "    print(\"üíæ Saved to data/interactions.parquet\")\n",
    "\n",
    "    return interaction_df\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# EXECUTION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "custom_rules = [\n",
    "    \"Premium products require demonstrated high transaction volumes\",\n",
    "    \"Savings products are suitable for customers with stable income\"\n",
    "]\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "interaction_df = create_customer_product_interactions(\n",
    "    df_customers,\n",
    "    df_products,\n",
    "    openai_client=client,\n",
    "    model_name=CONFIG[\"llm\"][\"model\"],\n",
    "    customer_sample_size=10,\n",
    "    batch_size=30,\n",
    "    temperature=CONFIG[\"llm\"][\"temperature\"],\n",
    "    top_n_products=CONFIG[\"recommendation\"][\"top_n\"],\n",
    "    rate_limit_delay=0.5,\n",
    "    additional_rules=custom_rules,\n",
    "    test_mode=False\n",
    ")\n",
    "\n",
    "print(\"\\nSample Interactions:\")\n",
    "interaction_df.show(10)\n",
    "\n",
    "print(\"\\nScore Distribution:\")\n",
    "interaction_df.describe([\"interaction_score\"]).show()\n",
    "\n",
    "print(\"\\nTop Products by Interaction Count:\")\n",
    "top_products = interaction_df.groupBy(\"Product_Name\").count().orderBy(F.desc(\"count\")).limit(10)\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c7f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ ALS RECOMMENDATIONS - PYSPARK VERSION\n",
      "======================================================================\n",
      "\n",
      "üìã Step 1: Loading data from CSV files...\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/c:/Users/adeye/Documents/Pinnacle-AI/notebooks/Downloads/interaction_pd.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 48\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 1) LOAD DATA\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìã Step 1: Loading data from CSV files...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m interaction_df \u001b[38;5;241m=\u001b[39m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m---> 48\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERACTIONS_PATH\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Basic validation\u001b[39;00m\n\u001b[0;32m     51\u001b[0m required_cols \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomer_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct_Name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteraction_score\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\adeye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:410\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    408\u001b[0m     path \u001b[38;5;241m=\u001b[39m [path]\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32mc:\\Users\\adeye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\adeye\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/c:/Users/adeye/Documents/Pinnacle-AI/notebooks/Downloads/interaction_pd.csv"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PYSPARK NOTEBOOK/SCRIPT ‚Äî ALS RECOMMENDATIONS FROM CSV\n",
    "# ============================================================\n",
    "# Requirements:\n",
    "#   - PySpark 3.4+ (recommended 3.5.x)\n",
    "#   - JDK 11/17 (JAVA_HOME set)\n",
    "#\n",
    "# Input:\n",
    "#   interaction_pd.csv   (required) columns: Customer_ID, Product_Name, interaction_score\n",
    "#   products.csv         (optional)\n",
    "#   customers.csv        (optional)\n",
    "#\n",
    "# Outputs (folders with CSVs):\n",
    "#   als_recommendations_csv/\n",
    "#   recommendations_pivot_csv/\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, row_number, desc, explode, first, count, lit\n",
    ")\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "TOP_N = 5\n",
    "INTERACTIONS_PATH = \"Downloads/interaction_pd.csv\"\n",
    "PRODUCTS_PATH = \"products.csv\"\n",
    "CUSTOMERS_PATH = \"customers.csv\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"ALS-Recommendations\")\n",
    "         # .config(\"spark.sql.shuffle.partitions\",\"200\")  # tune for cluster size\n",
    "         .getOrCreate())\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ ALS RECOMMENDATIONS - PYSPARK VERSION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) LOAD DATA\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüìã Step 1: Loading data from CSV files...\")\n",
    "\n",
    "interaction_df = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(INTERACTIONS_PATH))\n",
    "\n",
    "# Basic validation\n",
    "required_cols = {\"Customer_ID\", \"Product_Name\", \"interaction_score\"}\n",
    "missing = required_cols - set(interaction_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"{INTERACTIONS_PATH} missing columns: {missing}\")\n",
    "\n",
    "# Drop bad rows (nulls / non-numeric ratings)\n",
    "interaction_df = (interaction_df\n",
    "                  .where(col(\"Customer_ID\").isNotNull() & col(\"Product_Name\").isNotNull())\n",
    "                  .withColumn(\"interaction_score\", col(\"interaction_score\").cast(\"double\"))\n",
    "                  .where(col(\"interaction_score\").isNotNull()))\n",
    "\n",
    "print(f\"‚úÖ Loaded interactions: {interaction_df.count():,} rows\")\n",
    "print(f\"  ‚Ä¢ Unique customers: {interaction_df.select('Customer_ID').distinct().count():,}\")\n",
    "print(f\"  ‚Ä¢ Unique products : {interaction_df.select('Product_Name').distinct().count():,}\")\n",
    "\n",
    "def try_load(path):\n",
    "    try:\n",
    "        df = (spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(path))\n",
    "        print(f\"‚úÖ Loaded {path}: {df.count():,} rows\")\n",
    "        return df\n",
    "    except Exception:\n",
    "        print(f\"‚ÑπÔ∏è {path} not found (optional)\")\n",
    "        return None\n",
    "\n",
    "products_df  = try_load(PRODUCTS_PATH)\n",
    "customers_df = try_load(CUSTOMERS_PATH)\n",
    "\n",
    "# Keep only highest score per (Customer_ID, Product_Name)\n",
    "w_dedup = Window.partitionBy(\"Customer_ID\", \"Product_Name\").orderBy(desc(\"interaction_score\"))\n",
    "interaction_df = (interaction_df\n",
    "    .withColumn(\"rn\", row_number().over(w_dedup))\n",
    "    .where(col(\"rn\")==1)\n",
    "    .drop(\"rn\"))\n",
    "\n",
    "print(f\"üìä After de-dup: {interaction_df.count():,} interactions\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) INDEX STRING KEYS ‚Üí INTS FOR ALS\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüìã Step 2: Indexing IDs for ALS...\")\n",
    "\n",
    "user_indexer = StringIndexer(inputCol=\"Customer_ID\", outputCol=\"user_idx\", handleInvalid=\"skip\")\n",
    "item_indexer = StringIndexer(inputCol=\"Product_Name\", outputCol=\"item_idx\", handleInvalid=\"skip\")\n",
    "\n",
    "user_indexer_model = user_indexer.fit(interaction_df)\n",
    "item_indexer_model = item_indexer.fit(interaction_df)\n",
    "\n",
    "idx_df = user_indexer_model.transform(interaction_df)\n",
    "idx_df = item_indexer_model.transform(idx_df)\n",
    "\n",
    "idx_df = (idx_df\n",
    "          .withColumn(\"user_int\", col(\"user_idx\").cast(\"int\"))\n",
    "          .withColumn(\"item_int\", col(\"item_idx\").cast(\"int\"))\n",
    "          .select(\"Customer_ID\",\"Product_Name\",\"interaction_score\",\"user_int\",\"item_int\"))\n",
    "\n",
    "users_map = idx_df.select(\"user_int\",\"Customer_ID\").distinct()\n",
    "items_map = idx_df.select(\"item_int\",\"Product_Name\").distinct()\n",
    "\n",
    "print(f\"  ‚Ä¢ Indexed users:  {users_map.count():,}\")\n",
    "print(f\"  ‚Ä¢ Indexed items:  {items_map.count():,}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) TRAIN / EVALUATE ALS (explicit ratings)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüî® Step 3: Training ALS model...\")\n",
    "\n",
    "train_df, test_df = idx_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"user_int\",\n",
    "    itemCol=\"item_int\",\n",
    "    ratingCol=\"interaction_score\",\n",
    "    implicitPrefs=False,   # explicit ratings\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\",\n",
    "    rank=20,\n",
    "    regParam=0.1,\n",
    "    maxIter=20,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "model = als.fit(train_df)\n",
    "\n",
    "# Evaluate\n",
    "pred_test = model.transform(test_df)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"interaction_score\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(pred_test)\n",
    "print(f\"‚úÖ Trained. Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) RECOMMENDATIONS (Top-N per user) ‚Äî FIXED FIELD NAME\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüìã Step 4: Generating recommendations...\")\n",
    "\n",
    "recs = model.recommendForAllUsers(TOP_N)\n",
    "\n",
    "# If you're ever unsure, uncomment this to inspect schema:\n",
    "# recs.printSchema()\n",
    "# root\n",
    "#  |-- user_int: integer (nullable = false)\n",
    "#  |-- recommendations: array (nullable = true)\n",
    "#  |    |-- element: struct (containsNull = true)\n",
    "#  |    |    |-- item_int: integer (nullable = true)\n",
    "#  |    |    |-- rating: float (nullable = true)\n",
    "\n",
    "flat = (\n",
    "    recs\n",
    "    .select(col(\"user_int\"), explode(\"recommendations\").alias(\"rec\"))\n",
    "    .select(\n",
    "        col(\"user_int\"),\n",
    "        col(\"rec.item_int\").alias(\"item_int\"),        # <- NOTE: item_int (not itemInt)\n",
    "        col(\"rec.rating\").alias(\"recommendation_score\")\n",
    "    )\n",
    ")\n",
    "\n",
    "full_recs = (\n",
    "    flat\n",
    "    .join(users_map, on=\"user_int\", how=\"left\")\n",
    "    .join(items_map, on=\"item_int\", how=\"left\")\n",
    ")\n",
    "\n",
    "w_rank = Window.partitionBy(\"user_int\").orderBy(desc(\"recommendation_score\"))\n",
    "final_recs = (full_recs\n",
    "              .withColumn(\"rank\", row_number().over(w_rank))\n",
    "              .select(\"Customer_ID\",\"Product_Name\",\"recommendation_score\",\"rank\"))\n",
    "\n",
    "print(f\"‚úÖ Generated {final_recs.count():,} recommendations\")\n",
    "print(f\"   for {final_recs.select('Customer_ID').distinct().count():,} customers\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) INSIGHTS (Top products) & SAMPLE PRINTS\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüèÜ Most Recommended Products:\")\n",
    "(final_recs\n",
    " .groupBy(\"Product_Name\").agg(count(\"*\").alias(\"times_recommended\"))\n",
    " .orderBy(desc(\"times_recommended\"))\n",
    " .show(10, truncate=False))\n",
    "\n",
    "print(\"\\nüë• Sample customer recommendations:\")\n",
    "sample_ids = [r[\"Customer_ID\"] for r in final_recs.select(\"Customer_ID\").distinct().limit(3).collect()]\n",
    "for cid in sample_ids:\n",
    "    print(f\"\\nCustomer: {cid}\")\n",
    "    (final_recs\n",
    "     .where(col(\"Customer_ID\")==cid)\n",
    "     .orderBy(\"rank\")\n",
    "     .show(truncate=False))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) SAVE RESULTS\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüíæ Saving results...\")\n",
    "\n",
    "# a) Full recommendations (like als_recommendations.csv)\n",
    "(final_recs\n",
    " .orderBy(\"Customer_ID\",\"rank\")\n",
    " .coalesce(1)                          # For small-to-medium outputs; remove for big data\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"header\", True)\n",
    " .csv(\"als_recommendations_csv\"))\n",
    "\n",
    "# b) Pivot: Customer_ID x Recommendation_1..N\n",
    "from pyspark.sql.functions import expr\n",
    "pivot = (final_recs\n",
    "         .groupBy(\"Customer_ID\")\n",
    "         .pivot(\"rank\", list(range(1, TOP_N+1)))\n",
    "         .agg(first(\"Product_Name\"))\n",
    "         .orderBy(\"Customer_ID\"))\n",
    "\n",
    "for i in range(1, TOP_N+1):\n",
    "    pivot = pivot.withColumnRenamed(str(i), f\"Recommendation_{i}\")\n",
    "\n",
    "(pivot\n",
    " .coalesce(1)\n",
    " .write.mode(\"overwrite\")\n",
    " .option(\"header\", True)\n",
    " .csv(\"recommendations_pivot_csv\"))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALS RECOMMENDATION PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nFolders created:\")\n",
    "print(\"  ‚Ä¢ als_recommendations_csv/\")\n",
    "print(\"  ‚Ä¢ recommendations_pivot_csv/\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) (Optional) Popularity fallback for cold-start users/items\n",
    "# ------------------------------------------------------------\n",
    "# Example: create a simple popularity table you can join when needed\n",
    "popularity = (idx_df.groupBy(\"Product_Name\")\n",
    "              .agg(count(lit(1)).alias(\"interactions\"))\n",
    "              .orderBy(desc(\"interactions\")))\n",
    "popularity.show(10, truncate=False)\n",
    "\n",
    "# spark.stop()  # uncomment if running as a script\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
