{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66151545-6618-4ff7-87bf-5c43054f71eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install -q openai python-dotenv implicit scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857e59cf-1e30-4abc-ab03-86abc361421e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37fe4c41-319e-4ce5-96dd-ba10dc8d0478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, ndcg_score\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "import openai\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import json\n",
    "from decimal import Decimal\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2848b856-5a07-4a52-afd9-ca4db8a7a3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Configuration:\n",
      "{\n",
      "  \"als\": {\n",
      "    \"factors\": 50,\n",
      "    \"regularization\": 0.01,\n",
      "    \"iterations\": 15,\n",
      "    \"alpha\": 40.0\n",
      "  },\n",
      "  \"llm\": {\n",
      "    \"model\": \"gpt-4o\",\n",
      "    \"temperature\": 0.3,\n",
      "    \"max_tokens\": 2000\n",
      "  },\n",
      "  \"recommendation\": {\n",
      "    \"top_n\": 5,\n",
      "    \"final_n\": 3\n",
      "  },\n",
      "  \"feature_engineering\": {\n",
      "    \"recency_days\": 90,\n",
      "    \"min_transactions\": 3\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load OpenAI API key from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\n",
    "        \"âŒ OPENAI_API_KEY not found in environment variables.\\n\"\n",
    "        \"Please set it in your .env file or system environment.\\n\"\n",
    "        \"Create a .env file in the project root with: OPENAI_API_KEY=your_key_here\"\n",
    "    )\n",
    "\n",
    "print(f\"âœ… API key loaded from environment (key starts with: {openai_api_key[:20]}...)\")\n",
    "\n",
    "CONFIG = {\n",
    "    'als': {\n",
    "        'factors': 50,\n",
    "        'regularization': 0.01,\n",
    "        'iterations': 15,\n",
    "        'alpha': 40.0\n",
    "    },\n",
    "    'llm': {\n",
    "        'model': 'gpt-4o',\n",
    "        'temperature': 0.3,\n",
    "        'max_tokens': 2000\n",
    "    },\n",
    "    'recommendation': {\n",
    "        'top_n': 5,\n",
    "        'final_n': 3\n",
    "    },\n",
    "    'feature_engineering': {\n",
    "        'recency_days': 90,\n",
    "        'min_transactions': 3\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“Š Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0feb82da-efa3-45fb-a3a7-d08d3168528f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading datasets from Unity Catalog...\n",
      "\n",
      "Loading: workspace.default.transaction_data_1\n",
      "âœ… Loaded 125,549 transactions\n",
      "\n",
      "Loading: workspace.default.zenith_bank_product_catalog\n",
      "âœ… Loaded 42 products\n",
      "\n",
      "Loading: workspace.default.conversation_data_1\n",
      "âœ… Loaded 3,446 conversations\n",
      "\n",
      "Loading: workspace.default.customer_data_1\n",
      "âœ… Loaded 1,000 customer demographics\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š DATASET OVERVIEW\n",
      "================================================================================\n",
      "Unique Customers: 1,000\n",
      "Unique Products: 42\n",
      "Date Range: 2025-08-15 to 2025-11-13\n",
      "Total Transaction Value: â‚¦19,040,566,404.07\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DATA LOADING - Databricks Unity Catalog Tables (SPARK VERSION)\n",
    "# ==============================================================================\n",
    "# Update these table names to match your Unity Catalog tables\n",
    "# Format: catalog.schema.table_name\n",
    "\n",
    "TRANSACTIONS_TABLE = 'workspace.default.transaction_data_1'\n",
    "PRODUCTS_TABLE = 'workspace.default.zenith_bank_product_catalog'\n",
    "CONVERSATIONS_TABLE = 'workspace.default.conversation_data_1'\n",
    "CUSTOMERS_TABLE = 'workspace.default.customer_data_1'  # Optional\n",
    "\n",
    "# ==============================================================================\n",
    "# For CSV files in DBFS, use this format instead:\n",
    "# TRANSACTIONS_PATH = '/dbfs/FileStore/tables/zenith_bank_transaction.csv'\n",
    "# Then use: df_transactions = spark.read.csv(TRANSACTIONS_PATH, header=True, inferSchema=True)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"ðŸ“‚ Loading datasets from Unity Catalog...\\n\")\n",
    "\n",
    "# Load data using Spark (NO pandas conversion for distributed processing)\n",
    "try:\n",
    "    # Load transactions\n",
    "    print(f\"Loading: {TRANSACTIONS_TABLE}\")\n",
    "    df_transactions = spark.table(TRANSACTIONS_TABLE)\n",
    "    transactions_count = df_transactions.count()\n",
    "    print(f\"âœ… Loaded {transactions_count:,} transactions\")\n",
    "    \n",
    "    # Load products\n",
    "    print(f\"\\nLoading: {PRODUCTS_TABLE}\")\n",
    "    df_products = spark.table(PRODUCTS_TABLE)\n",
    "    products_count = df_products.count()\n",
    "    print(f\"âœ… Loaded {products_count:,} products\")\n",
    "    \n",
    "    # Load conversations\n",
    "    print(f\"\\nLoading: {CONVERSATIONS_TABLE}\")\n",
    "    df_conversations = spark.table(CONVERSATIONS_TABLE)\n",
    "    conversations_count = df_conversations.count()\n",
    "    print(f\"âœ… Loaded {conversations_count:,} conversations\")\n",
    "    \n",
    "    # Try loading customer demographics (optional)\n",
    "    print(f\"\\nLoading: {CUSTOMERS_TABLE}\")\n",
    "    try:\n",
    "        df_customers = spark.table(CUSTOMERS_TABLE)\n",
    "        customers_count = df_customers.count()\n",
    "        print(f\"âœ… Loaded {customers_count:,} customer demographics\")\n",
    "    except Exception as e:\n",
    "        df_customers = None\n",
    "        print(f\"âš ï¸ Customer demographics table not found: {e}\")\n",
    "        print(\"   Proceeding without demographics data\")\n",
    "    \n",
    "    # Display basic info using Spark operations\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š DATASET OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Unique Customers: {df_transactions.select('Customer_ID').distinct().count():,}\")\n",
    "    print(f\"Unique Products: {df_products.select('Product_ID').distinct().count():,}\")\n",
    "    \n",
    "    # Get date range\n",
    "    date_stats = df_transactions.agg(\n",
    "        F.min('Date').alias('min_date'),\n",
    "        F.max('Date').alias('max_date')\n",
    "    ).collect()[0]\n",
    "    print(f\"Date Range: {date_stats['min_date']} to {date_stats['max_date']}\")\n",
    "    \n",
    "    # Get total transaction value\n",
    "    total_value = df_transactions.agg(F.sum('Trans_Amount').alias('total')).collect()[0]['total']\n",
    "    print(f\"Total Transaction Value: â‚¦{total_value:,.2f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error loading data: {e}\")\n",
    "    print(\"\\nðŸ’¡ Troubleshooting:\")\n",
    "    print(\"   1. Verify table names exist in Unity Catalog\")\n",
    "    print(\"   2. Check you have READ permissions on these tables\")\n",
    "    print(\"   3. Run: spark.sql('SHOW TABLES IN workspace.default').show()\")\n",
    "    print(\"   4. Or run: spark.catalog.listTables('workspace.default')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f56766-cf4a-4254-8670-241e8943a874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š TRANSACTIONS SAMPLE:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Customer_ID='ZB000001', Trans_Amount=30126.97, Date=datetime.date(2025, 10, 1), Destination='Bookshop', Deb_or_credit='D', Narration='textbooks', Tran_Id='TR0000024', Category='Books', Account_Type='Aspire Account')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¦ PRODUCTS SAMPLE:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Product_ID='PROD001', Product_Name='Zenith Childrens Account (ZECA)', Product_Category='Savings Account', Age_Range='0-15 years', Opening_Balance='0', Minimum_Balance='0', Maximum_Balance='No limit', Currency='Naira, Dollar', Interest_Rate='Competitive', Monthly_Fee='0', Account_Maintenance_Fee='0', Key_Features='Zero opening balance, Scholarship opportunities, Education loan, Customized prepaid card, Annual Children Parade invitation', Digital_Channels='Zenith Mobile App, *966# EazyBanking, ZIVA', Card_Type='Customized Prepaid Card (upon parent request)', Target_Audience='Children aged 0-15 years', Special_Benefits='Teaches children to save, Parents can save for child future', Status='Active')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¥ CUSTOMERS SAMPLE:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Customer_ID='ZB000001', Full_Name='Engr. Adewale', First_Name='Adewale', Last_Name='Yusuf', Gender='Male', Date_of_Birth=datetime.date(1994, 10, 14), Age=31, Phone_Number=80472958637, Email='adewale.yusuf@outlook.com', Address='24 Awolowo Ave', City='New Lagos', State='Kwara', Occupation='Civil Servant', Employment_Status='Employed', Income_Bracket='â‚¦1,000,000+', Marital_Status='Single', Education_Level='BSc', Account_Type='Aspire Account', Account_Creation_Date=6080000000.0, BVN=93774863057, Status='Active')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¬ CONVERSATIONS SAMPLE:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(Conversation_ID='CONV00001', Customer_ID='ZB000001', Agent_ID='AGT90', Category='Complaints', Customer_Message='My debit card is not working', Customer_Sentiment='#######', Agent_Response='You can request via the mobile app', Agent_Time='#######', Channel='Phone Call', Status='Resolved', Satisfaction_Score=3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ TRANSACTION STATISTICS:\n",
      "DataFrame[summary: string, Customer_ID: string, Trans_Amount: string, Destination: string, Deb_or_credit: string, Narration: string, Tran_Id: string, Category: string, Account_Type: string]\n",
      "\n",
      "ðŸ” Missing Values:\n",
      "+-----------+------------+----+-----------+-------------+---------+-------+--------+------------+\n",
      "|Customer_ID|Trans_Amount|Date|Destination|Deb_or_credit|Narration|Tran_Id|Category|Account_Type|\n",
      "+-----------+------------+----+-----------+-------------+---------+-------+--------+------------+\n",
      "|          0|           0|   0|          0|            0|        0|      0|       0|           0|\n",
      "+-----------+------------+----+-----------+-------------+---------+-------+--------+------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Quick data exploration\n",
    "print(\"\\nðŸ“Š TRANSACTIONS SAMPLE:\")\n",
    "display(df_transactions.head())\n",
    "\n",
    "print(\"\\nðŸ¦ PRODUCTS SAMPLE:\")\n",
    "display(df_products.head())\n",
    "\n",
    "print(\"\\nðŸ‘¥ CUSTOMERS SAMPLE:\")\n",
    "display(df_customers.head())\n",
    "\n",
    "print(\"\\nðŸ’¬ CONVERSATIONS SAMPLE:\")\n",
    "display(df_conversations.head())\n",
    "\n",
    "print(\"\\nðŸ“ˆ TRANSACTION STATISTICS:\")\n",
    "print(df_transactions.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nðŸ” Missing Values:\")\n",
    "print(df_transactions.select([F.sum(F.col(c).isNull().cast('int')).alias(c) for c in df_transactions.columns]).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff8919b8-2b26-4ddb-81dc-c77a6da64da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting Hybrid interaction matrix creation...\n",
      "\n",
      "ðŸ”¨ Creating interaction matrix (Hybrid: OpenAI + Transaction Descriptions)...\n",
      "\n",
      "   Products in catalog: 42\n",
      "\n",
      "   â†’ Identifying current products from Account_Type...\n",
      "      Found current products for 571 customers\n",
      "\n",
      "   â†’ Preparing customer profiles...\n",
      "      Total customers: 1,000\n",
      "      Processing all 1,000 customers\n",
      "\n",
      "   â†’ Preparing product catalog and deriving rules from data...\n",
      "   â†’ Deriving business rules from product catalog...\n",
      "      Generated 86 product-specific rules from catalog\n",
      "\n",
      "   â†’ Using OpenAI to score product fit (intelligent matching)...\n",
      "      This may take time depending on sample size...\n",
      "      Processing batch 1/34... âœ“ (150 matches)\n",
      "      Processing batch 2/34... âœ“ (150 matches)\n",
      "      Processing batch 3/34... âœ“ (150 matches)\n",
      "      Processing batch 4/34... âœ“ (150 matches)\n",
      "      Processing batch 5/34... âœ“ (150 matches)\n",
      "      Processing batch 6/34... âœ“ (150 matches)\n",
      "      Processing batch 7/34... âœ“ (150 matches)\n",
      "      Processing batch 8/34... âœ“ (150 matches)\n",
      "      Processing batch 9/34... âœ“ (150 matches)\n",
      "      Processing batch 10/34..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'application/vnd.databricks.toolz-hint+json': {'trigger': 'hungCommand'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python Execution Stuck!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ“ (150 matches)\n",
      "      Processing batch 11/34... âœ“ (150 matches)\n",
      "      Processing batch 12/34... âœ“ (150 matches)\n",
      "      Processing batch 13/34... âœ“ (150 matches)\n",
      "      Processing batch 14/34... âœ“ (150 matches)\n",
      "      Processing batch 15/34... âœ“ (150 matches)\n",
      "      Processing batch 16/34... âœ“ (150 matches)\n",
      "      Processing batch 17/34... âœ“ (150 matches)\n",
      "      Processing batch 18/34... âœ“ (150 matches)\n",
      "      Processing batch 19/34... âœ“ (150 matches)\n",
      "      Processing batch 20/34... âœ“ (150 matches)\n",
      "      Processing batch 21/34... âœ“ (150 matches)\n",
      "      Processing batch 22/34... âœ“ (150 matches)\n",
      "      Processing batch 23/34... âœ“ (150 matches)\n",
      "      Processing batch 24/34... âœ“ (150 matches)\n",
      "      Processing batch 25/34... âœ“ (150 matches)\n",
      "      Processing batch 26/34... âœ“ (150 matches)\n",
      "      Processing batch 27/34... âœ“ (150 matches)\n",
      "      Processing batch 28/34... âœ“ (150 matches)\n",
      "      Processing batch 29/34... âœ“ (150 matches)\n",
      "      Processing batch 30/34... âœ“ (150 matches)\n",
      "      Processing batch 31/34... âœ“ (150 matches)\n",
      "      Processing batch 32/34..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'application/vnd.databricks.toolz-hint+json': {'trigger': 'hungCommand'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python Execution Stuck!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " âœ“ (150 matches)\n",
      "      Processing batch 33/34... âœ“ (150 matches)\n",
      "      Processing batch 34/34... âœ“ (50 matches)\n",
      "\n",
      "   â†’ Computing transaction-based interaction scores...\n",
      "      Analyzing transaction descriptions for product signals...\n",
      "      Filtered to 125,549 transactions for sampled customers\n",
      "      âš  Description field not found - using Account_Type only\n",
      "      Matched 157,512 transactions to products\n",
      "      Computing RFM (Recency, Frequency, Monetary) scores...\n",
      "      Generated 1,231 transaction-based interactions\n",
      "\n",
      "      Sample RFM scores:\n",
      "+-----------+-----------------------------+-----------------+-----------------+--------------------+---------------+\n",
      "|Customer_ID|Product_Name                 |trans_score      |transaction_count|total_amount        |days_since_last|\n",
      "+-----------+-----------------------------+-----------------+-----------------+--------------------+---------------+\n",
      "|ZB000094   |Aspire Account               |10.0             |151              |2.588046261E7       |0              |\n",
      "|ZB000891   |Timeless Current Account     |9.73271722053662 |70               |1.5297729570000002E7|2              |\n",
      "|ZB000588   |Zenith Salary Savings Account|10.0             |182              |3.2451948740000017E7|0              |\n",
      "|ZB000671   |Timeless Current Account     |10.0             |182              |3.011019091000002E7 |0              |\n",
      "|ZB000281   |Timeless Current Account     |9.886840789585532|86               |1.246896757E7       |0              |\n",
      "+-----------+-----------------------------+-----------------+-----------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "   â†’ Merging transaction and OpenAI scores...\n",
      "      Combined into 6,032 total interactions\n",
      "      Weighting: 70% transactions + 30% OpenAI\n",
      "\n",
      "   â†’ Building final interaction matrix...\n",
      "\n",
      "   â†’ Filtering current products...\n",
      "\n",
      "================================================================================\n",
      "âœ… HYBRID INTERACTION MATRIX CREATED\n",
      "================================================================================\n",
      "Total interactions: 4,801\n",
      "Unique customers: 1,000\n",
      "Unique products: 30\n",
      "Products in catalog: 42\n",
      "\n",
      "ðŸ§  gpt-4o + Transaction Data:\n",
      "   â€¢ 86 rules derived from product catalog\n",
      "   â€¢ 2 custom business rules\n",
      "   â€¢ Customer demographics and attributes\n",
      "   â€¢ Real transaction history with descriptions (weight: 70%)\n",
      "   â€¢ RFM analysis: Recency (30%) + Frequency (35%) + Monetary (35%)\n",
      "\n",
      "âš ï¸ Current products EXCLUDED from recommendations\n",
      "   Customers with current products: 571\n",
      "\n",
      "âš¡ Completed in 2310.6 seconds\n",
      "   Model used: gpt-4o\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Sample Interactions:\n",
      "+-----------+--------------------+------------------+\n",
      "|Customer_ID|        Product_Name| interaction_score|\n",
      "+-----------+--------------------+------------------+\n",
      "|   ZB000057|        Z-Woman Loan|2.1000000000000005|\n",
      "|   ZB000074|Zenith Individual...|2.4000000000000004|\n",
      "|   ZB000201|Zenith Internet B...|2.4000000000000004|\n",
      "|   ZB000307|Zenith Internet B...|               2.7|\n",
      "|   ZB000471|Zenith Internet B...|               2.7|\n",
      "|   ZB000551|Zenith Internet B...|1.2000000000000002|\n",
      "|   ZB000736|Zenith Internet B...|2.1000000000000005|\n",
      "|   ZB000919|Zenith Internet B...|2.4000000000000004|\n",
      "|   ZB000028|Zenith Internet B...|2.4000000000000004|\n",
      "|   ZB000744| Zenith Prepaid Card|1.5000000000000002|\n",
      "+-----------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "ðŸŽ¯ Score Distribution:\n",
      "+-------+------------------+\n",
      "|summary| interaction_score|\n",
      "+-------+------------------+\n",
      "|  count|              4801|\n",
      "|   mean|2.2540928973130665|\n",
      "| stddev|0.3286722260879667|\n",
      "|    min|1.2000000000000002|\n",
      "|    max|3.0000000000000004|\n",
      "+-------+------------------+\n",
      "\n",
      "\n",
      "ðŸ“ˆ Top Products by Interaction Count:\n",
      "+--------------------+-----+\n",
      "|        Product_Name|count|\n",
      "+--------------------+-----+\n",
      "|Zenith Mobile Ban...|  955|\n",
      "|Zenith Internet B...|  878|\n",
      "|Zenith Individual...|  697|\n",
      "|Zenith Debit Card...|  473|\n",
      "|Zenith Credit Car...|  340|\n",
      "|Zenith Gold Premi...|  281|\n",
      "|Zenith Credit Car...|  193|\n",
      "|Zenith Salary Sav...|  113|\n",
      "|   *966# EazyBanking|  113|\n",
      "|Zenith Debit Card...|   77|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6 - HYBRID INTERACTION MATRIX (OPENAI + TRANSACTIONS WITH DESCRIPTIONS)\n",
    "# ============================================================================\n",
    "\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import json\n",
    "from decimal import Decimal\n",
    "\n",
    "def create_customer_product_interactions(df_custs, df_products, df_trans,\n",
    "                                          openai_api_key='**REDACTED**', \n",
    "                                          model_name=\"gpt-4o-mini\",\n",
    "                                          customer_sample_size=None,\n",
    "                                          batch_size=30,\n",
    "                                          temperature=0.3,\n",
    "                                          top_n_products=10,\n",
    "                                          rate_limit_delay=0.5,\n",
    "                                          additional_rules=None,\n",
    "                                          use_transaction_data=True,\n",
    "                                          transaction_weight=0.7):\n",
    "    \"\"\"\n",
    "    HYBRID: Combines OpenAI intelligent matching with real transaction data\n",
    "    Enhanced with transaction description analysis\n",
    "    All parameters configurable, no hard-coded values\n",
    "    Uses Spark for distributed processing\n",
    "    \n",
    "    Args:\n",
    "        df_custs: Customer DataFrame with demographics and Account_Type\n",
    "        df_products: Product catalog DataFrame\n",
    "        df_trans: Transaction DataFrame (with Description field)\n",
    "        openai_api_key: OpenAI API key (required)\n",
    "        model_name: OpenAI model to use\n",
    "        customer_sample_size: Number of customers to process (None = all)\n",
    "        batch_size: Number of customers per API call\n",
    "        temperature: LLM temperature setting\n",
    "        top_n_products: Number of products to recommend per customer\n",
    "        rate_limit_delay: Delay between API calls in seconds\n",
    "        additional_rules: Optional list of custom business rules\n",
    "        use_transaction_data: Whether to include transaction data\n",
    "        transaction_weight: Weight for transaction scores (0-1)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"ðŸ”¨ Creating interaction matrix (Hybrid: OpenAI + Transaction Descriptions)...\\n\")\n",
    "    \n",
    "    # Validate inputs\n",
    "    if openai_api_key is None:\n",
    "        raise ValueError(\"openai_api_key parameter is required\")\n",
    "    \n",
    "    # Initialize OpenAI client\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    # Get all product names\n",
    "    all_product_names = [row.Product_Name for row in df_products.select('Product_Name').collect()]\n",
    "    print(f\"   Products in catalog: {len(all_product_names)}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 1. IDENTIFY CURRENT PRODUCTS FROM ACCOUNT_TYPE (SPARK)\n",
    "    # =========================================================================\n",
    "    print(\"\\n   â†’ Identifying current products from Account_Type...\")\n",
    "    \n",
    "    df_custs_with_keywords = df_custs.withColumn(\n",
    "        'account_keywords',\n",
    "        F.regexp_extract(F.lower(F.col('Account_Type')), \n",
    "                        r'(current|savings|sme|student|premium|platinum|children|aspire)', 1)\n",
    "    )\n",
    "    \n",
    "    df_products_keywords = df_products.withColumn(\n",
    "        'product_keywords',\n",
    "        F.regexp_extract(F.lower(F.col('Product_Name')), \n",
    "                        r'(current|savings|sme|student|premium|platinum|children|aspire)', 1)\n",
    "    )\n",
    "    \n",
    "    keyword_map = df_products_keywords.filter(F.col('product_keywords') != '') \\\n",
    "        .select('product_keywords', 'Product_Name') \\\n",
    "        .distinct()\n",
    "    \n",
    "    df_custs_with_current = df_custs_with_keywords.join(\n",
    "        keyword_map,\n",
    "        df_custs_with_keywords.account_keywords == keyword_map.product_keywords,\n",
    "        'left'\n",
    "    ).withColumnRenamed('Product_Name', 'current_product')\n",
    "    \n",
    "    customer_current_products_df = df_custs_with_current.filter(\n",
    "        F.col('current_product').isNotNull()\n",
    "    ).groupBy('Customer_ID').agg(\n",
    "        F.collect_set('current_product').alias('current_products')\n",
    "    )\n",
    "    \n",
    "    current_products_count = customer_current_products_df.count()\n",
    "    print(f\"      Found current products for {current_products_count:,} customers\")\n",
    "    \n",
    "    customer_current_products = {\n",
    "        row.Customer_ID: set(row.current_products) \n",
    "        for row in customer_current_products_df.collect()\n",
    "    }\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 2. PREPARE CUSTOMER PROFILES (SPARK)\n",
    "    # =========================================================================\n",
    "    print(\"\\n   â†’ Preparing customer profiles...\")\n",
    "    \n",
    "    customer_profiles = df_custs.select([c for c in df_custs.columns])\n",
    "    profiles_count = customer_profiles.count()\n",
    "    print(f\"      Total customers: {profiles_count:,}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 3. SAMPLE CUSTOMERS IF SPECIFIED\n",
    "    # =========================================================================\n",
    "    if customer_sample_size is not None and profiles_count > customer_sample_size:\n",
    "        print(f\"      Sampling {customer_sample_size:,} customers from {profiles_count:,} total\")\n",
    "        customer_profiles_sample = customer_profiles.sample(\n",
    "            fraction=customer_sample_size/profiles_count, \n",
    "            seed=42\n",
    "        ).limit(customer_sample_size)\n",
    "    else:\n",
    "        print(f\"      Processing all {profiles_count:,} customers\")\n",
    "        customer_profiles_sample = customer_profiles\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 4. PREPARE PRODUCT CATALOG WITH RULES (SPARK)\n",
    "    # =========================================================================\n",
    "    print(\"\\n   â†’ Preparing product catalog and deriving rules from data...\")\n",
    "    \n",
    "    product_cols = [c for c in df_products.columns]\n",
    "    product_data = df_products.select(product_cols).collect()\n",
    "    \n",
    "    def safe_convert(value):\n",
    "        if value is None:\n",
    "            return None\n",
    "        if isinstance(value, Decimal):\n",
    "            return float(value)\n",
    "        if isinstance(value, (int, float, str, bool)):\n",
    "            return value\n",
    "        return str(value)\n",
    "    \n",
    "    product_catalog = []\n",
    "    for row in product_data:\n",
    "        product_info = {'name': row.Product_Name}\n",
    "        for col in product_cols:\n",
    "            if col != 'Product_Name' and hasattr(row, col):\n",
    "                value = getattr(row, col)\n",
    "                converted_value = safe_convert(value)\n",
    "                if converted_value is not None:\n",
    "                    product_info[col.lower()] = converted_value\n",
    "        product_catalog.append(product_info)\n",
    "    \n",
    "    product_list_parts = []\n",
    "    for i, prod in enumerate(product_catalog):\n",
    "        parts = [f\"{i+1}. {prod['name']}\"]\n",
    "        if 'product_category' in prod:\n",
    "            parts.append(f\"Category: {prod['product_category']}\")\n",
    "        if 'target_audience' in prod:\n",
    "            parts.append(f\"Target: {prod['target_audience']}\")\n",
    "        if 'age_range' in prod:\n",
    "            parts.append(f\"Age Range: {prod['age_range']}\")\n",
    "        if 'minimum_balance' in prod:\n",
    "            try:\n",
    "                min_bal = float(prod['minimum_balance'])\n",
    "                parts.append(f\"Min Balance: â‚¦{min_bal:,.0f}\")\n",
    "            except (ValueError, TypeError):\n",
    "                parts.append(f\"Min Balance: {prod['minimum_balance']}\")\n",
    "        if 'interest_rate' in prod:\n",
    "            parts.append(f\"Interest Rate: {prod['interest_rate']}\")\n",
    "        for key, value in prod.items():\n",
    "            if key not in ['name', 'product_category', 'target_audience', 'age_range', 'minimum_balance', 'interest_rate']:\n",
    "                parts.append(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "        product_list_parts.append(\" - \".join(parts))\n",
    "    \n",
    "    product_list = \"\\n\".join(product_list_parts)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 5. BUILD DYNAMIC RULES FROM PRODUCT CATALOG\n",
    "    # =========================================================================\n",
    "    print(\"   â†’ Deriving business rules from product catalog...\")\n",
    "    \n",
    "    derived_rules = []\n",
    "    for prod in product_catalog:\n",
    "        prod_name = prod['name']\n",
    "        if 'age_range' in prod and prod['age_range']:\n",
    "            age_range = str(prod['age_range'])\n",
    "            derived_rules.append(f\"- {prod_name}: Only for customers within age range {age_range}\")\n",
    "        if 'target_audience' in prod and prod['target_audience']:\n",
    "            target = prod['target_audience']\n",
    "            derived_rules.append(f\"- {prod_name}: Designed for {target}\")\n",
    "        if 'minimum_balance' in prod and prod['minimum_balance']:\n",
    "            try:\n",
    "                min_bal = float(prod['minimum_balance'])\n",
    "                if min_bal > 0:\n",
    "                    derived_rules.append(f\"- {prod_name}: Requires minimum balance of â‚¦{min_bal:,.0f}\")\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "    \n",
    "    if additional_rules:\n",
    "        derived_rules.extend([f\"- {rule}\" for rule in additional_rules])\n",
    "    \n",
    "    general_rules = [\n",
    "        \"- Match products to customer demographics (age, income, occupation)\",\n",
    "        \"- Do NOT recommend products similar to customer's current account\",\n",
    "        \"- Consider customer's financial capacity when recommending products\",\n",
    "        \"- Respect all age ranges and target audience specifications STRICTLY\"\n",
    "    ]\n",
    "    \n",
    "    all_rules = general_rules + derived_rules\n",
    "    rules_text = \"\\n\".join(all_rules) if all_rules else \"- Match products appropriately to customer profiles\"\n",
    "    \n",
    "    print(f\"      Generated {len(derived_rules)} product-specific rules from catalog\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 6. USE OPENAI TO SCORE CUSTOMER-PRODUCT FIT\n",
    "    # =========================================================================\n",
    "    print(\"\\n   â†’ Using OpenAI to score product fit (intelligent matching)...\")\n",
    "    print(\"      This may take time depending on sample size...\")\n",
    "    \n",
    "    customer_profiles_list = customer_profiles_sample.collect()\n",
    "    total_batches = (len(customer_profiles_list) + batch_size - 1) // batch_size\n",
    "    all_interactions = []\n",
    "    \n",
    "    for batch_idx in range(0, len(customer_profiles_list), batch_size):\n",
    "        batch = customer_profiles_list[batch_idx:batch_idx + batch_size]\n",
    "        print(f\"      Processing batch {batch_idx//batch_size + 1}/{total_batches}...\", end='')\n",
    "        \n",
    "        customer_summaries = []\n",
    "        for cust in batch:\n",
    "            summary_parts = [f\"Customer {cust.Customer_ID}:\"]\n",
    "            for col in df_custs.columns:\n",
    "                if col != 'Customer_ID' and hasattr(cust, col):\n",
    "                    value = getattr(cust, col)\n",
    "                    converted_value = safe_convert(value)\n",
    "                    if converted_value is not None:\n",
    "                        summary_parts.append(f\"- {col.replace('_', ' ').title()}: {converted_value}\")\n",
    "            customer_summaries.append('\\n'.join(summary_parts))\n",
    "        \n",
    "        prompt = f\"\"\"You are a banking product recommendation expert. Score how well each product fits each customer on a scale of 0-10.\n",
    "\n",
    "# CUSTOMERS\n",
    "{chr(10).join(customer_summaries)}\n",
    "\n",
    "# PRODUCTS\n",
    "{product_list}\n",
    "\n",
    "# TASK\n",
    "For each customer, score ONLY the top {top_n_products} most relevant products (0-10 scale).\n",
    "\n",
    "# MATCHING RULES\n",
    "{rules_text}\n",
    "\n",
    "Return ONLY valid JSON with this exact structure:\n",
    "{{\n",
    "  \"matches\": [\n",
    "    {{\"customer_id\": \"C001\", \"product\": \"Exact Product Name\", \"score\": 8}},\n",
    "    {{\"customer_id\": \"C001\", \"product\": \"Another Product\", \"score\": 7}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Do NOT include any other text, only the JSON object.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a banking product expert. Return only valid JSON.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            response_text = response.choices[0].message.content.strip()\n",
    "            if response_text.startswith('```'):\n",
    "                response_text = response_text.split('```')[1]\n",
    "                if response_text.startswith('json'):\n",
    "                    response_text = response_text[4:]\n",
    "                response_text = response_text.strip()\n",
    "            \n",
    "            result = json.loads(response_text)\n",
    "            for match in result.get('matches', []):\n",
    "                all_interactions.append({\n",
    "                    'Customer_ID': match['customer_id'],\n",
    "                    'Product_Name': match['product'],\n",
    "                    'interaction_score': float(match['score'])\n",
    "                })\n",
    "            \n",
    "            print(f\" âœ“ ({len(result.get('matches', []))} matches)\")\n",
    "        except Exception as e:\n",
    "            print(f\" âœ— Error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if batch_idx + batch_size < len(customer_profiles_list):\n",
    "            time.sleep(rate_limit_delay)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 7. CREATE TRANSACTION-BASED INTERACTIONS (WITH DESCRIPTION ANALYSIS)\n",
    "    # =========================================================================\n",
    "    interaction_matrix = None\n",
    "    \n",
    "    if use_transaction_data and df_trans is not None:\n",
    "        print(\"\\n   â†’ Computing transaction-based interaction scores...\")\n",
    "        print(\"      Analyzing transaction descriptions for product signals...\")\n",
    "        \n",
    "        # Get sampled customer IDs\n",
    "        sampled_customer_ids = [row.Customer_ID for row in customer_profiles_sample.select('Customer_ID').collect()]\n",
    "        df_trans_filtered = df_trans.filter(F.col('Customer_ID').isin(sampled_customer_ids))\n",
    "        \n",
    "        trans_filtered_count = df_trans_filtered.count()\n",
    "        print(f\"      Filtered to {trans_filtered_count:,} transactions for sampled customers\")\n",
    "        \n",
    "        # Extract keywords from Account_Type\n",
    "        df_trans_with_keywords = df_trans_filtered.withColumn(\n",
    "            'account_keywords',\n",
    "            F.regexp_extract(F.lower(F.col('Account_Type')), \n",
    "                            r'(current|savings|sme|student|premium|platinum|children|aspire)', 1)\n",
    "        )\n",
    "        \n",
    "        # Extract keywords from Description (if it exists)\n",
    "        if 'Description' in df_trans_filtered.columns:\n",
    "            print(\"      âœ“ Description field found - using for enhanced matching\")\n",
    "            df_trans_with_keywords = df_trans_with_keywords.withColumn(\n",
    "                'desc_keywords',\n",
    "                F.regexp_extract(F.lower(F.col('Description')), \n",
    "                                r'(current|savings|sme|student|premium|platinum|children|aspire|loan|investment|card|transfer|deposit)', 1)\n",
    "            )\n",
    "            \n",
    "            # Combine keywords from both sources\n",
    "            df_trans_with_keywords = df_trans_with_keywords.withColumn(\n",
    "                'combined_keywords',\n",
    "                F.when(F.col('account_keywords') != '', F.col('account_keywords'))\n",
    "                 .when(F.col('desc_keywords') != '', F.col('desc_keywords'))\n",
    "                 .otherwise('')\n",
    "            )\n",
    "        else:\n",
    "            print(\"      âš  Description field not found - using Account_Type only\")\n",
    "            df_trans_with_keywords = df_trans_with_keywords.withColumn(\n",
    "                'combined_keywords',\n",
    "                F.col('account_keywords')\n",
    "            )\n",
    "        \n",
    "        # Match transactions to products using combined keywords\n",
    "        transaction_product_matches = df_trans_with_keywords.join(\n",
    "            keyword_map.withColumnRenamed('product_keywords', 'combined_keywords'),\n",
    "            'combined_keywords',\n",
    "            'inner'\n",
    "        )\n",
    "        \n",
    "        matches_count = transaction_product_matches.count()\n",
    "        print(f\"      Matched {matches_count:,} transactions to products\")\n",
    "        \n",
    "        # Aggregate transaction metrics (RFM Analysis)\n",
    "        transaction_interactions = transaction_product_matches.groupBy('Customer_ID', 'Product_Name').agg(\n",
    "            F.count('*').alias('transaction_count'),\n",
    "            F.sum('Trans_Amount').alias('total_amount'),\n",
    "            F.max('Date').alias('last_transaction_date'),\n",
    "            F.avg('Trans_Amount').alias('avg_amount')\n",
    "        )\n",
    "        \n",
    "        # Calculate recency\n",
    "        max_date = df_trans_filtered.agg(F.max('Date')).collect()[0][0]\n",
    "        \n",
    "        transaction_interactions = transaction_interactions.withColumn(\n",
    "            'days_since_last',\n",
    "            F.datediff(F.lit(max_date), F.col('last_transaction_date'))\n",
    "        )\n",
    "        \n",
    "        # Create RFM-based interaction score (0-10 scale)\n",
    "        print(\"      Computing RFM (Recency, Frequency, Monetary) scores...\")\n",
    "        \n",
    "        # Frequency score: normalize transaction count (log scale for better distribution)\n",
    "        transaction_interactions = transaction_interactions.withColumn(\n",
    "            'frequency_score',\n",
    "            F.least(F.lit(10.0), (F.log1p(F.col('transaction_count')) / F.log1p(F.lit(100.0))) * 10.0)\n",
    "        )\n",
    "        \n",
    "        # Monetary score: normalize total amount (log scale)\n",
    "        transaction_interactions = transaction_interactions.withColumn(\n",
    "            'monetary_score',\n",
    "            F.least(F.lit(10.0), (F.log1p(F.col('total_amount')) / F.log1p(F.lit(1000000.0))) * 10.0)\n",
    "        )\n",
    "        \n",
    "        # Recency score: decay based on days since last transaction\n",
    "        transaction_interactions = transaction_interactions.withColumn(\n",
    "            'recency_score',\n",
    "            F.when(F.col('days_since_last') <= 30, F.lit(10.0))\n",
    "             .when(F.col('days_since_last') <= 60, F.lit(8.0))\n",
    "             .when(F.col('days_since_last') <= 90, F.lit(6.0))\n",
    "             .when(F.col('days_since_last') <= 180, F.lit(4.0))\n",
    "             .when(F.col('days_since_last') <= 365, F.lit(2.0))\n",
    "             .otherwise(F.lit(0.5))\n",
    "        )\n",
    "        \n",
    "        # Combined RFM score with weights\n",
    "        transaction_interactions = transaction_interactions.withColumn(\n",
    "            'transaction_score',\n",
    "            (F.col('frequency_score') * 0.35) +   # 35% frequency\n",
    "            (F.col('monetary_score') * 0.35) +    # 35% monetary\n",
    "            (F.col('recency_score') * 0.30)       # 30% recency\n",
    "        )\n",
    "        \n",
    "        transaction_interactions = transaction_interactions.select(\n",
    "            'Customer_ID',\n",
    "            'Product_Name',\n",
    "            F.col('transaction_score').alias('trans_score'),\n",
    "            'transaction_count',\n",
    "            'total_amount',\n",
    "            'days_since_last'\n",
    "        )\n",
    "        \n",
    "        trans_count = transaction_interactions.count()\n",
    "        print(f\"      Generated {trans_count:,} transaction-based interactions\")\n",
    "        \n",
    "        # Show sample RFM scores\n",
    "        print(\"\\n      Sample RFM scores:\")\n",
    "        transaction_interactions.select(\n",
    "            'Customer_ID', 'Product_Name', 'trans_score', \n",
    "            'transaction_count', 'total_amount', 'days_since_last'\n",
    "        ).show(5, truncate=False)\n",
    "        \n",
    "        # =========================================================================\n",
    "        # 8. MERGE TRANSACTION AND OPENAI SCORES\n",
    "        # =========================================================================\n",
    "        print(\"\\n   â†’ Merging transaction and OpenAI scores...\")\n",
    "        \n",
    "        if not all_interactions:\n",
    "            print(\"      No OpenAI interactions, using transaction data only\")\n",
    "            interaction_matrix = transaction_interactions.select(\n",
    "                'Customer_ID',\n",
    "                'Product_Name',\n",
    "                F.col('trans_score').alias('interaction_score')\n",
    "            )\n",
    "        else:\n",
    "            openai_interactions = spark.createDataFrame(all_interactions)\n",
    "            \n",
    "            # Full outer join\n",
    "            combined = openai_interactions.join(\n",
    "                transaction_interactions.select('Customer_ID', 'Product_Name', 'trans_score'),\n",
    "                ['Customer_ID', 'Product_Name'],\n",
    "                'full_outer'\n",
    "            )\n",
    "            \n",
    "            # Combine scores with weighting\n",
    "            combined = combined.withColumn(\n",
    "                'openai_score',\n",
    "                F.coalesce(F.col('interaction_score'), F.lit(0.0))\n",
    "            ).withColumn(\n",
    "                'trans_score_filled',\n",
    "                F.coalesce(F.col('trans_score'), F.lit(0.0))\n",
    "            ).withColumn(\n",
    "                'interaction_score',\n",
    "                (F.col('trans_score_filled') * transaction_weight) + \n",
    "                (F.col('openai_score') * (1.0 - transaction_weight))\n",
    "            )\n",
    "            \n",
    "            interaction_matrix = combined.select(\n",
    "                'Customer_ID',\n",
    "                'Product_Name',\n",
    "                'interaction_score'\n",
    "            )\n",
    "            \n",
    "            combined_count = interaction_matrix.count()\n",
    "            print(f\"      Combined into {combined_count:,} total interactions\")\n",
    "            print(f\"      Weighting: {transaction_weight*100:.0f}% transactions + {(1-transaction_weight)*100:.0f}% OpenAI\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n   â†’ Using OpenAI scores only (no transaction data)\")\n",
    "        if all_interactions:\n",
    "            interaction_matrix = spark.createDataFrame(all_interactions)\n",
    "        else:\n",
    "            print(\"   âš ï¸ No interactions generated. Creating fallback...\")\n",
    "            fallback_count = min(3, len(all_product_names))\n",
    "            fallback_products = all_product_names[:fallback_count]\n",
    "            fallback_data = []\n",
    "            for cust_row in customer_profiles_sample.select('Customer_ID').collect():\n",
    "                for prod in fallback_products:\n",
    "                    fallback_data.append((cust_row.Customer_ID, prod, 5.0))\n",
    "            interaction_matrix = spark.createDataFrame(\n",
    "                fallback_data,\n",
    "                ['Customer_ID', 'Product_Name', 'interaction_score']\n",
    "            )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 9. AGGREGATE AND FINALIZE\n",
    "    # =========================================================================\n",
    "    print(\"\\n   â†’ Building final interaction matrix...\")\n",
    "    \n",
    "    interaction_matrix = interaction_matrix.groupBy('Customer_ID', 'Product_Name').agg(\n",
    "        F.max('interaction_score').alias('interaction_score')\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 10. FILTER OUT CURRENT PRODUCTS\n",
    "    # =========================================================================\n",
    "    print(\"\\n   â†’ Filtering current products...\")\n",
    "    \n",
    "    current_pairs = []\n",
    "    for cust, prods in customer_current_products.items():\n",
    "        for prod in prods:\n",
    "            current_pairs.append((cust, prod))\n",
    "    \n",
    "    if current_pairs:\n",
    "        current_pairs_df = spark.createDataFrame(\n",
    "            current_pairs,\n",
    "            ['Customer_ID', 'Product_Name']\n",
    "        )\n",
    "        interaction_matrix = interaction_matrix.join(\n",
    "            current_pairs_df,\n",
    "            ['Customer_ID', 'Product_Name'],\n",
    "            'left_anti'\n",
    "        )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FINAL STATS\n",
    "    # =========================================================================\n",
    "    total_interactions = interaction_matrix.count()\n",
    "    unique_customers = interaction_matrix.select('Customer_ID').distinct().count()\n",
    "    unique_products = interaction_matrix.select('Product_Name').distinct().count()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… HYBRID INTERACTION MATRIX CREATED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total interactions: {total_interactions:,}\")\n",
    "    print(f\"Unique customers: {unique_customers:,}\")\n",
    "    print(f\"Unique products: {unique_products:,}\")\n",
    "    print(f\"Products in catalog: {len(product_catalog)}\")\n",
    "    print(f\"\\nðŸ§  {model_name} + Transaction Data:\")\n",
    "    print(f\"   â€¢ {len(derived_rules)} rules derived from product catalog\")\n",
    "    print(f\"   â€¢ {len(additional_rules) if additional_rules else 0} custom business rules\")\n",
    "    print(f\"   â€¢ Customer demographics and attributes\")\n",
    "    if use_transaction_data:\n",
    "        print(f\"   â€¢ Real transaction history with descriptions (weight: {transaction_weight*100:.0f}%)\")\n",
    "        print(f\"   â€¢ RFM analysis: Recency (30%) + Frequency (35%) + Monetary (35%)\")\n",
    "    print(f\"\\nâš ï¸ Current products EXCLUDED from recommendations\")\n",
    "    print(f\"   Customers with current products: {len(customer_current_products):,}\")\n",
    "    print(f\"\\nâš¡ Completed in {elapsed:.1f} seconds\")\n",
    "    print(f\"   Model used: {model_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return interaction_matrix, df_products, customer_current_products\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸš€ Starting Hybrid interaction matrix creation...\\n\")\n",
    "\n",
    "custom_rules = [\n",
    "    \"Premium products require demonstrated high transaction volumes\",\n",
    "    \"Savings products are suitable for customers with stable income\"\n",
    "]\n",
    "\n",
    "interaction_df, product_map, customer_current_products = create_customer_product_interactions(\n",
    "    df_customers,\n",
    "    df_products,\n",
    "    df_transactions,                # Transaction data with descriptions\n",
    "    openai_api_key=openai_api_key,\n",
    "    model_name=CONFIG['llm']['model'],\n",
    "    customer_sample_size=1000,\n",
    "    batch_size=30,\n",
    "    temperature=CONFIG['llm']['temperature'],\n",
    "    top_n_products=CONFIG['recommendation']['top_n'],\n",
    "    rate_limit_delay=0.5,\n",
    "    additional_rules=custom_rules,\n",
    "    use_transaction_data=True,      # Enable transaction data\n",
    "    transaction_weight=0.7           # 70% transactions, 30% OpenAI\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Sample Interactions:\")\n",
    "interaction_df.show(10)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Score Distribution:\")\n",
    "interaction_df.describe(['interaction_score']).show()\n",
    "\n",
    "print(\"\\nðŸ“ˆ Top Products by Interaction Count:\")\n",
    "top_products = interaction_df.groupBy('Product_Name').count().orderBy(F.desc('count')).limit(10)\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccda00c8-91a1-4266-8ccf-200a402ecb25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "interaction_df=interaction_df.toPandas()\n",
    "interaction_df.to_csv(\"interaction_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9662bf08-ebe2-4d12-b4e8-cb3efaa18e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting customer feature engineering...\n",
      "\n",
      "ðŸ”¨ Engineering customer features for 1000 customers (Spark)...\n",
      "   â†’ Sampling 1,000 customers from customer table...\n",
      "      âœ“ Sampled 1,000 customers\n",
      "      âœ“ Filtered to 125,549 transactions\n",
      "      âœ“ Filtered to 3,446 conversations\n",
      "   â†’ Preparing transaction data...\n",
      "   â†’ Computing date range features...\n",
      "   â†’ Computing aggregated features...\n",
      "   â†’ Computing category preferences...\n",
      "   â†’ Merging all features...\n",
      "   â†’ Computing derived features...\n",
      "   â†’ Adding conversation features...\n",
      "   â†’ Adding demographic features...\n",
      "   â†’ Computing final derived features...\n",
      "âœ… Engineered 1,000 customer profiles with 33 features\n",
      "   âš¡ Completed in 1.95 seconds (513 customers/sec)\n",
      "\n",
      "ðŸ“Š Customer Features Sample:\n",
      "+-----------+--------------------+--------------------+------------------+------------------+------------------+-----------------+-----------+------------+-----------------+-------------------+------------------------+--------------------+--------------------------+--------------+------------+-------------------+----------------------+-------------------+------------------+--------------------------+---------------------------+------------------+--------------------+-------------+---+-----------------+---------------------+-----------+------+------------------+--------------------+------------------+\n",
      "|Customer_ID|total_debit         |total_credit        |avg_transaction   |std_transaction   |median_transaction|transaction_count|debit_count|credit_count|unique_categories|unique_destinations|recent_transaction_count|recent_debit        |current_account           |date_span_days|top_category|top_category_amount|category_concentration|net_balance        |debit_credit_ratio|transaction_frequency_days|days_since_last_transaction|conversation_count|top_inquiry_category|message_count|Age|Occupation       |Income_Bracket       |State      |Gender|financial_velocity|spending_consistency|engagement_score  |\n",
      "+-----------+--------------------+--------------------+------------------+------------------+------------------+-----------------+-----------+------------+-----------------+-------------------+------------------------+--------------------+--------------------------+--------------+------------+-------------------+----------------------+-------------------+------------------+--------------------------+---------------------------+------------------+--------------------+-------------+---+-----------------+---------------------+-----------+------+------------------+--------------------+------------------+\n",
      "|ZB000227   |1.1261307439999998E7|1.0027468579999998E7|137346.94206451616|226361.26372737755|31402.51          |155              |137        |18          |10               |25                 |155                     |1.1261307439999998E7|Savings Account           |89            |Utilities   |5657820.9799999995 |0.15328467153284672   |-1233838.8599999994|1.1230458963951198|1.7415730337078652        |89                         |2                 |USSD Banking        |2            |63 |HR Manager       |â‚¦1,000,000+          |Enugu      |Male  |1.7222222222222223|1.648086315126945   |95.0              |\n",
      "|ZB000683   |7813527.349999999   |7583253.5200000005  |160383.1340625    |260596.72937541438|31597.66          |96               |87         |9           |10               |25                 |96                      |7813527.349999999   |Zenith Premier Account    |88            |Education   |3559072.8600000003 |0.13793103448275862   |-230273.8299999982 |1.03036609937841  |1.0909090909090908        |88                         |5                 |Cheque Service      |5            |31 |Accountant       |â‚¦100,000 - â‚¦500,000  |Ogun       |Male  |1.0786516853932584|1.624828608507264   |75.9              |\n",
      "|ZB000384   |6258287.979999999   |3990761.8000000003  |122012.49738095242|210012.31438233587|31765.01          |84               |77         |7           |10               |25                 |84                      |6258287.979999999   |Savings Account           |90            |Books       |2576639.05         |0.18181818181818182   |-2267526.1799999983|1.5681938170301215|0.9333333333333333        |90                         |4                 |Loan Inquiry        |4            |54 |Nurse            |â‚¦500,000 - â‚¦1,000,000|Cross River|Female|0.9230769230769231|1.7212219868317697  |69.6              |\n",
      "|ZB000290   |5636361.310000004   |2165301.58          |127896.11295081976|227294.3689148887 |33920.47          |61               |58         |3           |11               |23                 |61                      |5636361.310000004   |Savings Account           |87            |Utilities   |3421280.41         |0.1724137931034483    |-3471059.730000004 |2.6030375454674557|0.7011494252873564        |87                         |4                 |Transaction Query   |4            |42 |Doctor           |â‚¦1,000,000+          |Sokoto     |Male  |0.6931818181818182|1.7771657519923076  |63.400000000000006|\n",
      "|ZB000826   |1.2563531619999997E7|1.3698333689999998E7|159162.8200606061 |272534.03899004083|34658.7           |165              |146        |19          |10               |25                 |165                     |1.2563531619999997E7|Save4me Account           |89            |Wedding     |4958414.89         |0.15753424657534246   |1134802.0700000003 |0.917157656129488 |1.853932584269663         |89                         |3                 |Transaction Query   |3            |56 |Marketing Manager|â‚¦500,000 - â‚¦1,000,000|Abuja FCT  |Female|1.8333333333333333|1.7122863656216962  |100.5             |\n",
      "|ZB000413   |2741617.010000001   |3144896.05          |98108.55100000002 |203587.69111902147|22858.12          |60               |55         |5           |10               |20                 |60                      |2741617.010000001   |Individual Current Account|90            |Groceries   |1074865.45         |0.2545454545454545    |403279.03999999864 |0.8717671320169712|0.6666666666666666        |90                         |2                 |Internet Banking    |2            |55 |Nurse            |â‚¦500,000 - â‚¦1,000,000|Ogun       |Male  |0.6593406593406593|2.075105726648585   |57.0              |\n",
      "|ZB000759   |1.4144980749999994E7|1.3793891070000004E7|214914.39861538462|285580.24949732446|69371.8           |130              |108        |22          |10               |24                 |130                     |1.4144980749999994E7|Timeless Account          |90            |Books       |9086192.25         |0.16666666666666666   |-351089.6799999904 |1.0254525483939456|1.4444444444444444        |90                         |4                 |Complaints          |4            |27 |Software Engineer|â‚¦500,000 - â‚¦1,000,000|Enugu      |Female|1.4285714285714286|1.328803107349244   |88.0              |\n",
      "|ZB000196   |1.0438270059999999E7|1.6119435E7         |183156.5866206897 |283061.04948630044|30342.1           |145              |120        |25          |10               |25                 |145                     |1.0438270059999999E7|Timeless Account          |89            |Dining      |5818656.39         |0.15833333333333333   |5681164.940000001  |0.6475580601925562|1.6292134831460674        |89                         |5                 |Cheque Service      |5            |40 |Pharmacist       |â‚¦1,000,000+          |Abuja FCT  |Female|1.6111111111111112|1.5454508585140176  |95.5              |\n",
      "|ZB000389   |9208768.379999997   |1.647145576E7       |174695.4023129251 |280141.77384589886|37058.97          |147              |123        |24          |10               |25                 |147                     |9208768.379999997   |Current Account           |89            |Wedding     |3326164.79         |0.15447154471544716   |7262687.380000003  |0.5590743474151794|1.651685393258427         |89                         |3                 |Card Service        |3            |52 |HR Manager       |â‚¦1,000,000+          |Cross River|Male  |1.6333333333333333|1.6035921183087367  |93.30000000000001 |\n",
      "|ZB000474   |4123255.9500000007  |4913288.43          |148140.07180327867|233308.86380359868|26501.15          |61               |51         |10          |10               |21                 |61                      |4123255.9500000007  |Aspire Account            |89            |Dining      |2159593.1900000004 |0.21568627450980393   |790032.479999999   |0.8392049456783063|0.6853932584269663        |89                         |5                 |Transaction Query   |5            |64 |Architect        |â‚¦500,000 - â‚¦1,000,000|Anambra    |Male  |0.6777777777777778|1.574910056769517   |61.900000000000006|\n",
      "+-----------+--------------------+--------------------+------------------+------------------+------------------+-----------------+-----------+------------+-----------------+-------------------+------------------------+--------------------+--------------------------+--------------+------------+-------------------+----------------------+-------------------+------------------+--------------------------+---------------------------+------------------+--------------------+-------------+---+-----------------+---------------------+-----------+------+------------------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "ðŸ“ˆ Feature Statistics:\n",
      "+-------+-----------+--------------------+--------------------+------------------+------------------+------------------+-----------------+-----------------+-----------------+------------------+-------------------+------------------------+--------------------+---------------+-----------------+------------+--------------------+----------------------+--------------------+------------------+--------------------------+---------------------------+------------------+--------------------+------------------+------------------+----------+--------------------+---------+------+-------------------+--------------------+------------------+\n",
      "|summary|Customer_ID|         total_debit|        total_credit|   avg_transaction|   std_transaction|median_transaction|transaction_count|      debit_count|     credit_count| unique_categories|unique_destinations|recent_transaction_count|        recent_debit|current_account|   date_span_days|top_category| top_category_amount|category_concentration|         net_balance|debit_credit_ratio|transaction_frequency_days|days_since_last_transaction|conversation_count|top_inquiry_category|     message_count|               Age|Occupation|      Income_Bracket|    State|Gender| financial_velocity|spending_consistency|  engagement_score|\n",
      "+-------+-----------+--------------------+--------------------+------------------+------------------+------------------+-----------------+-----------------+-----------------+------------------+-------------------+------------------------+--------------------+---------------+-----------------+------------+--------------------+----------------------+--------------------+------------------+--------------------------+---------------------------+------------------+--------------------+------------------+------------------+----------+--------------------+---------+------+-------------------+--------------------+------------------+\n",
      "|  count|       1000|                1000|                1000|              1000|              1000|              1000|             1000|             1000|             1000|              1000|               1000|                    1000|                1000|           1000|             1000|        1000|                1000|                  1000|                1000|              1000|                      1000|                       1000|              1000|                1000|              1000|              1000|      1000|                1000|     1000|  1000|               1000|                1000|              1000|\n",
      "|   mean|       NULL|   9877194.965940002|    9163371.43813001|151615.01300560663|244147.68729960918|36432.615300000005|          125.549|          111.204|           14.345|            10.501|             25.082|                 125.549|   9877194.965940002|           NULL|           89.122|        NULL|   5022617.810640006|   0.15617363807120743|  -713823.5278100009|1.1154371849903342|        1.4059982688209511|                     89.122|             3.446|                NULL|             3.446|            44.289|      NULL|                NULL|     NULL|  NULL| 1.3904247416336475|  1.6241381808133817| 86.89160000000007|\n",
      "| stddev|       NULL|   3756597.022834836|   4640238.853723482| 28321.53901778244| 38174.47689690415| 10327.71572277382|43.84095834523497|38.94361294216276|7.011928560827056|0.6696621890531435| 1.7738157802946797|       43.84095834523497|   3756597.022834836|           NULL|1.295683138269077|        NULL|   2349014.878915033|  0.022346799645255776|  3902240.5765103847|0.7660483481022826|       0.48445190562615414|          1.295683138269077|1.1001337620009213|                NULL|1.1001337620009213|12.299949909156854|      NULL|                NULL|     NULL|  NULL|0.47915695479367293|  0.1259599198726189|17.800532250143203|\n",
      "|    min|   ZB000001|  2313020.2200000007|                 0.0|54603.078076923084|105532.34546387145|          17804.45|               50|               41|                0|                 8|                 19|                      50|  2313020.2200000007| Aspire Account|               79|       Books|           760939.62|   0.11009174311926606|-1.95675450099999...|               0.0|        0.5555555555555556|                         79|                 2|     Account Balance|                 2|                22|Accountant|          â‚¦0,100,000|Abuja FCT|Female| 0.5494505494505495|  1.2571929201262495|51.900000000000006|\n",
      "|    max|   ZB001000|2.0477857580000006E7|2.6114257089999996E7|252020.50366666663|351241.99810739246|           97731.4|              200|              198|               37|                11|                 27|                     200|2.0477857580000006E7|   Zeta Account|               90|     Wedding|1.3656580409999998E7|   0.25757575757575757| 1.342088622999999E7|14.510524160144678|         2.235955056179775|                         90|                 5|        USSD Banking|                 5|                65|    Trader|â‚¦500,000 - â‚¦1,000...|   Sokoto|  Male| 2.2111111111111112|  2.2287133440655813|120.10000000000001|\n",
      "+-------+-----------+--------------------+--------------------+------------------+------------------+------------------+-----------------+-----------------+-----------------+------------------+-------------------+------------------------+--------------------+---------------+-----------------+------------+--------------------+----------------------+--------------------+------------------+--------------------------+---------------------------+------------------+--------------------+------------------+------------------+----------+--------------------+---------+------+-------------------+--------------------+------------------+\n",
      "\n",
      "\n",
      "ðŸ“‹ Feature Columns:\n",
      "Total columns: 33\n",
      "  - Customer_ID\n",
      "  - total_debit\n",
      "  - total_credit\n",
      "  - avg_transaction\n",
      "  - std_transaction\n",
      "  - median_transaction\n",
      "  - transaction_count\n",
      "  - debit_count\n",
      "  - credit_count\n",
      "  - unique_categories\n",
      "  - unique_destinations\n",
      "  - recent_transaction_count\n",
      "  - recent_debit\n",
      "  - current_account\n",
      "  - date_span_days\n",
      "  - top_category\n",
      "  - top_category_amount\n",
      "  - category_concentration\n",
      "  - net_balance\n",
      "  - debit_credit_ratio\n",
      "  - transaction_frequency_days\n",
      "  - days_since_last_transaction\n",
      "  - conversation_count\n",
      "  - top_inquiry_category\n",
      "  - message_count\n",
      "  - Age\n",
      "  - Occupation\n",
      "  - Income_Bracket\n",
      "  - State\n",
      "  - Gender\n",
      "  - financial_velocity\n",
      "  - spending_consistency\n",
      "  - engagement_score\n"
     ]
    }
   ],
   "source": [
    "def engineer_customer_features(df_trans, df_convs, df_custs=None, recency_days=90, customer_sample_size=1000):\n",
    "    \"\"\"\n",
    "    Create comprehensive customer features for ML and LLM context.\n",
    "    SPARK OPTIMIZED: Uses distributed processing\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from pyspark.sql import Window\n",
    "    from pyspark.sql.functions import col, count, sum as spark_sum, mean, stddev, expr\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"ðŸ”¨ Engineering customer features for {customer_sample_size if customer_sample_size else 'ALL'} customers (Spark)...\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SAMPLE CUSTOMERS FIRST (from df_custs - SOURCE TABLE)\n",
    "    # =========================================================================\n",
    "    if customer_sample_size and df_custs is not None:\n",
    "        print(f\"   â†’ Sampling {customer_sample_size:,} customers from customer table...\")\n",
    "        total_customers = df_custs.count()\n",
    "        sample_fraction = customer_sample_size / total_customers\n",
    "        df_custs_sample = df_custs.sample(fraction=sample_fraction, seed=42).limit(customer_sample_size)\n",
    "        sampled_customer_ids = [row.Customer_ID for row in df_custs_sample.select('Customer_ID').collect()]\n",
    "        sampled_customer_ids_set = set(sampled_customer_ids)\n",
    "        \n",
    "        # Filter transactions and conversations to only include sampled customers\n",
    "        df_trans = df_trans.filter(col('Customer_ID').isin(sampled_customer_ids))\n",
    "        df_convs = df_convs.filter(col('Customer_ID').isin(sampled_customer_ids))\n",
    "        df_custs = df_custs_sample\n",
    "        \n",
    "        print(f\"      âœ“ Sampled {len(sampled_customer_ids):,} customers\")\n",
    "        print(f\"      âœ“ Filtered to {df_trans.count():,} transactions\")\n",
    "        print(f\"      âœ“ Filtered to {df_convs.count():,} conversations\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PREPARE TRANSACTION DATA\n",
    "    # =========================================================================\n",
    "    print(\"   â†’ Preparing transaction data...\")\n",
    "    \n",
    "    # Ensure Date is date type\n",
    "    df_trans = df_trans.withColumn('Date', F.to_date(F.col('Date')))\n",
    "    \n",
    "    # Create debit/credit flags\n",
    "    df_trans = df_trans.withColumn('is_debit', F.when(F.col('Deb_or_credit') == 'D', 1).otherwise(0))\n",
    "    df_trans = df_trans.withColumn('is_credit', F.when(F.col('Deb_or_credit') == 'C', 1).otherwise(0))\n",
    "    df_trans = df_trans.withColumn('debit_amount', F.col('Trans_Amount') * F.col('is_debit'))\n",
    "    df_trans = df_trans.withColumn('credit_amount', F.col('Trans_Amount') * F.col('is_credit'))\n",
    "    \n",
    "    # Calculate recency\n",
    "    max_date = df_trans.agg(F.max('Date')).collect()[0][0]\n",
    "    recent_threshold = max_date - timedelta(days=recency_days)\n",
    "    \n",
    "    df_trans = df_trans.withColumn('is_recent', F.when(F.col('Date') >= F.lit(recent_threshold), 1).otherwise(0))\n",
    "    df_trans = df_trans.withColumn('recent_debit', F.col('debit_amount') * F.col('is_recent'))\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 1. DATE RANGE FEATURES\n",
    "    # =========================================================================\n",
    "    print(\"   â†’ Computing date range features...\")\n",
    "    \n",
    "    date_ranges = df_trans.groupBy('Customer_ID').agg(\n",
    "        F.min('Date').alias('min_date'),\n",
    "        F.max('Date').alias('max_date')\n",
    "    )\n",
    "    \n",
    "    date_ranges = date_ranges.withColumn(\n",
    "        'date_span_days',\n",
    "        F.when(F.datediff(F.col('max_date'), F.col('min_date')) == 0, 1)\n",
    "         .otherwise(F.datediff(F.col('max_date'), F.col('min_date')))\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 2. AGGREGATED FEATURES\n",
    "    # =========================================================================\n",
    "    print(\"   â†’ Computing aggregated features...\")\n",
    "    \n",
    "    agg_features = df_trans.groupBy('Customer_ID').agg(\n",
    "        # Financial metrics\n",
    "        F.sum('debit_amount').alias('total_debit'),\n",
    "        F.sum('credit_amount').alias('total_credit'),\n",
    "        F.mean('Trans_Amount').alias('avg_transaction'),\n",
    "        F.stddev('Trans_Amount').alias('std_transaction'),\n",
    "        F.expr('percentile_approx(Trans_Amount, 0.5)').alias('median_transaction'),\n",
    "        F.count('Trans_Amount').alias('transaction_count'),\n",
    "        \n",
    "        # Debit/Credit counts\n",
    "        F.sum('is_debit').alias('debit_count'),\n",
    "        F.sum('is_credit').alias('credit_count'),\n",
    "        \n",
    "        # Diversity metrics\n",
    "        F.countDistinct('Category').alias('unique_categories'),\n",
    "        F.countDistinct('Destination').alias('unique_destinations'),\n",
    "        \n",
    "        # Recency\n",
    "        F.sum('is_recent').alias('recent_transaction_count'),\n",
    "        F.sum('recent_debit').alias('recent_debit'),\n",
    "        \n",
    "        # Account type (most common using first)\n",
    "        F.first('Account_Type').alias('current_account')\n",
    "    )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 3. TOP CATEGORY FEATURES\n",
    "    # =========================================================================\n",
    "    print(\"   â†’ Computing category preferences...\")\n",
    "    \n",
    "    # Filter debit transactions\n",
    "    debit_txns = df_trans.filter(F.col('is_debit') == 1)\n",
    "    \n",
    "    # Top category per customer (most frequent)\n",
    "    window_spec = Window.partitionBy('Customer_ID', 'Category')\n",
    "    category_counts = debit_txns.groupBy('Customer_ID', 'Category').agg(\n",
    "        F.count('*').alias('category_count')\n",
    "    )\n",
    "    \n",
    "    window_rank = Window.partitionBy('Customer_ID').orderBy(F.desc('category_count'))\n",
    "    top_categories = category_counts.withColumn('rank', F.row_number().over(window_rank)) \\\n",
    "        .filter(F.col('rank') == 1) \\\n",
    "        .select('Customer_ID', F.col('Category').alias('top_category'))\n",
    "    \n",
    "    # Top category spending amount\n",
    "    category_amounts = debit_txns.groupBy('Customer_ID', 'Category').agg(\n",
    "        F.sum('Trans_Amount').alias('category_amount')\n",
    "    )\n",
    "    \n",
    "    window_rank_amount = Window.partitionBy('Customer_ID').orderBy(F.desc('category_amount'))\n",
    "    top_category_amounts = category_amounts.withColumn('rank', F.row_number().over(window_rank_amount)) \\\n",
    "        .filter(F.col('rank') == 1) \\\n",
    "        .select('Customer_ID', F.col('category_amount').alias('top_category_amount'))\n",
    "    \n",
    "    # Category concentration (simplified - top category count / total count)\n",
    "    category_concentration = category_counts.join(\n",
    "        debit_txns.groupBy('Customer_ID').agg(F.count('*').alias('total_count')),\n",
    "        'Customer_ID'\n",
    "    ).withColumn('category_concentration', F.col('category_count') / F.col('total_count'))\n",
    "    \n",
    "    window_rank_conc = Window.partitionBy('Customer_ID').orderBy(F.desc('category_concentration'))\n",
    "    category_concentration = category_concentration.withColumn('rank', F.row_number().over(window_rank_conc)) \\\n",
    "        .filter(F.col('rank') == 1) \\\n",
    "        .select('Customer_ID', 'category_concentration')\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 4. MERGE BASE FEATURES\n",
    "    # =========================================================================\n",
    "    print(\"   â†’ Merging all features...\")\n",
    "    \n",
    "    df_features = agg_features.join(date_ranges.select('Customer_ID', 'date_span_days'), 'Customer_ID', 'left')\n",
    "    df_features = df_features.join(top_categories, 'Customer_ID', 'left')\n",
    "    df_features = df_features.join(top_category_amounts, 'Customer_ID', 'left')\n",
    "    df_features = df_features.join(category_concentration, 'Customer_ID', 'left')\n",
    "    \n",
    "    # Fill nulls\n",
    "    df_features = df_features.fillna({\n",
    "        'std_transaction': 0,\n",
    "        'top_category': 'Unknown',\n",
    "        'top_category_amount': 0,\n",
    "        'category_concentration': 0\n",
    "    })\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 5. COMPUTED FEATURES\n",
    "    # =========================================================================\n",
    "    print(\"   â†’ Computing derived features...\")\n",
    "    \n",
    "    df_features = df_features.withColumn('net_balance', F.col('total_credit') - F.col('total_debit'))\n",
    "    df_features = df_features.withColumn(\n",
    "        'debit_credit_ratio',\n",
    "        F.when(F.col('total_credit') > 0, F.col('total_debit') / F.col('total_credit')).otherwise(0)\n",
    "    )\n",
    "    df_features = df_features.withColumn(\n",
    "        'transaction_frequency_days',\n",
    "        F.col('transaction_count') / F.col('date_span_days')\n",
    "    )\n",
    "    df_features = df_features.withColumn('days_since_last_transaction', F.col('date_span_days'))\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 6. CONVERSATION FEATURES\n",
    "    # =========================================================================\n",
    "    print(\"   â†’ Adding conversation features...\")\n",
    "    \n",
    "    conv_count = df_convs.count()\n",
    "    if conv_count > 0:\n",
    "        conv_features = df_convs.groupBy('Customer_ID').agg(\n",
    "            F.count('Category').alias('conversation_count'),\n",
    "            F.first('Category').alias('top_inquiry_category'),\n",
    "            F.count('Customer_Message').alias('message_count')\n",
    "        )\n",
    "        df_features = df_features.join(conv_features, 'Customer_ID', 'left')\n",
    "        df_features = df_features.fillna({\n",
    "            'conversation_count': 0,\n",
    "            'message_count': 0,\n",
    "            'top_inquiry_category': 'None'\n",
    "        })\n",
    "    else:\n",
    "        df_features = df_features.withColumn('conversation_count', F.lit(0))\n",
    "        df_features = df_features.withColumn('message_count', F.lit(0))\n",
    "        df_features = df_features.withColumn('top_inquiry_category', F.lit('None'))\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 7. DEMOGRAPHIC FEATURES (from df_custs - SOURCE TABLE)\n",
    "    # =========================================================================\n",
    "    print(\"   â†’ Adding demographic features...\")\n",
    "    \n",
    "    if df_custs is not None:\n",
    "        demo_cols = ['Customer_ID']\n",
    "        available_cols = df_custs.columns\n",
    "        \n",
    "        if 'Age' in available_cols: demo_cols.append('Age')\n",
    "        if 'Occupation' in available_cols: demo_cols.append('Occupation')\n",
    "        if 'Income_Bracket' in available_cols: demo_cols.append('Income_Bracket')\n",
    "        if 'State' in available_cols: demo_cols.append('State')\n",
    "        if 'Gender' in available_cols: demo_cols.append('Gender')\n",
    "        if 'Location' in available_cols: demo_cols.append('Location')\n",
    "        \n",
    "        demo_features = df_custs.select(demo_cols)\n",
    "        df_features = df_features.join(demo_features, 'Customer_ID', 'left')\n",
    "    \n",
    "    # =========================================================================\n",
    "    # 8. FINAL DERIVED FEATURES\n",
    "    # =========================================================================\n",
    "    print(\"   â†’ Computing final derived features...\")\n",
    "    \n",
    "    df_features = df_features.withColumn(\n",
    "        'financial_velocity',\n",
    "        F.col('transaction_count') / (F.col('days_since_last_transaction') + 1)\n",
    "    )\n",
    "    df_features = df_features.withColumn(\n",
    "        'spending_consistency',\n",
    "        F.col('std_transaction') / (F.col('avg_transaction') + 1)\n",
    "    )\n",
    "    df_features = df_features.withColumn(\n",
    "        'engagement_score',\n",
    "        (F.col('transaction_count') * 0.4) + \n",
    "        (F.col('unique_categories') * 10 * 0.3) + \n",
    "        (F.col('conversation_count') * 5 * 0.3)\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    feature_count = df_features.count()\n",
    "    col_count = len(df_features.columns)\n",
    "    \n",
    "    print(f\"âœ… Engineered {feature_count:,} customer profiles with {col_count} features\")\n",
    "    print(f\"   âš¡ Completed in {elapsed_time:.2f} seconds ({feature_count/elapsed_time:.0f} customers/sec)\")\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ðŸš€ Starting customer feature engineering...\\n\")\n",
    "\n",
    "customer_features = engineer_customer_features(\n",
    "    df_transactions,      # Transactions table (filtered by sampled customers)\n",
    "    df_conversations,     # Conversations table (filtered by sampled customers)\n",
    "    df_customers,         # CUSTOMERS TABLE - SOURCE TABLE for sampling\n",
    "    recency_days=CONFIG['feature_engineering']['recency_days'],\n",
    "    customer_sample_size=1000  # Sample 1000 customers\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Customer Features Sample:\")\n",
    "customer_features.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Feature Statistics:\")\n",
    "customer_features.describe().show()\n",
    "\n",
    "print(\"\\nðŸ“‹ Feature Columns:\")\n",
    "print(f\"Total columns: {len(customer_features.columns)}\")\n",
    "for col in customer_features.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0662b3-1b14-4ce8-8157-e23d15ab1600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_features1=customer_features.toPandas()\n",
    "customer_features1.to_csv(\"customer_features.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f3170c-37ac-45c7-a435-89461b364b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Step 1: Preparing data...\n",
      "âœ… Cleaned: 4,801 interactions\n",
      "\n",
      "ðŸ“‹ Step 2: Creating indexes...\n",
      "  âœ“ Customers: 1,000\n",
      "  âœ“ Products: 30\n",
      "\n",
      "ðŸ”¨ Step 3: Training ALS...\n",
      "âœ… Trained. RMSE: 0.2467\n",
      "\n",
      "ðŸ“‹ Step 4: Generating recommendations...\n",
      "  âœ“ Users: 999, Items: 29\n",
      "  âœ“ Computed 4,995 recommendations\n",
      "\n",
      "ðŸ’¾ Saving ALS table...\n",
      "âœ… Table 1 saved: als_recommendations_table\n",
      "\n",
      "ðŸ¤– Step 5: Generating LLM explanations (limiting to 20 customers)...\n",
      "  â†’ Selecting top 20 customers...\n",
      "  âœ“ Processing 20 customers, 60 recommendations...\n",
      "  â†’ Fetching customer features...\n",
      "  âœ“ Loaded 20 customer profiles\n",
      "\n",
      "  ðŸ”„ Starting LLM generation at 13:08:23...\n",
      "  â†’ Progress: 1/60 (1%) | Elapsed: 0.0s | Est. remaining: 0.0s\n",
      "    âœ“ Generated explanation for ZB000071 - Timeless Savings Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000071 - Aspire Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000071 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000101 - Timeless Savings Account (Rank 3)\n",
      "  â†’ Progress: 5/60 (8%) | Elapsed: 9.8s | Est. remaining: 107.3s\n",
      "    âœ“ Generated explanation for ZB000101 - Aspire Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000101 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000121 - Timeless Savings Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000121 - Timeless Current Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000121 - Aspire Account (Rank 1)\n",
      "  â†’ Progress: 10/60 (16%) | Elapsed: 29.2s | Est. remaining: 146.2s\n",
      "    âœ“ Generated explanation for ZB000161 - Timeless Savings Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000161 - Aspire Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000161 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000162 - Aspire Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000162 - Timeless Savings Account (Rank 2)\n",
      "  â†’ Progress: 15/60 (25%) | Elapsed: 43.0s | Est. remaining: 129.1s\n",
      "    âœ“ Generated explanation for ZB000162 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000192 - Timeless Savings Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000192 - Aspire Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000192 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000242 - Timeless Savings Account (Rank 3)\n",
      "  â†’ Progress: 20/60 (33%) | Elapsed: 60.7s | Est. remaining: 121.4s\n",
      "    âœ“ Generated explanation for ZB000242 - Aspire Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000242 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000371 - Timeless Savings Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000371 - Aspire Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000371 - Timeless Current Account (Rank 1)\n",
      "  â†’ Progress: 25/60 (41%) | Elapsed: 79.4s | Est. remaining: 111.2s\n",
      "    âœ“ Generated explanation for ZB000451 - Timeless Savings Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000451 - Aspire Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000451 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000511 - Timeless Savings Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000511 - Aspire Account (Rank 2)\n",
      "  â†’ Progress: 30/60 (50%) | Elapsed: 92.7s | Est. remaining: 92.7s\n",
      "    âœ“ Generated explanation for ZB000511 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000521 - Timeless Savings Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000521 - Aspire Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000521 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000541 - Aspire Account (Rank 3)\n",
      "  â†’ Progress: 35/60 (58%) | Elapsed: 109.3s | Est. remaining: 78.1s\n",
      "    âœ“ Generated explanation for ZB000541 - Timeless Savings Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000541 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000551 - Zenith Credit Card - Platinum/Infinite (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000551 - Zenith Platinum Premium Current Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000551 - Zenith Gold Premium Current Account (Rank 1)\n",
      "  â†’ Progress: 40/60 (66%) | Elapsed: 121.9s | Est. remaining: 60.9s\n",
      "    âœ“ Generated explanation for ZB000561 - Timeless Current Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000561 - Zenith Platinum Premium Current Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000561 - Zenith Gold Premium Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000571 - Timeless Savings Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000571 - Aspire Account (Rank 2)\n",
      "  â†’ Progress: 45/60 (75%) | Elapsed: 133.2s | Est. remaining: 44.4s\n",
      "    âœ“ Generated explanation for ZB000571 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000671 - Timeless Savings Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000671 - Aspire Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000671 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000721 - Timeless Savings Account (Rank 3)\n",
      "  â†’ Progress: 50/60 (83%) | Elapsed: 146.2s | Est. remaining: 29.2s\n",
      "    âœ“ Generated explanation for ZB000721 - Aspire Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000721 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000761 - Aspire Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000761 - Timeless Savings Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000761 - Timeless Current Account (Rank 1)\n",
      "  â†’ Progress: 55/60 (91%) | Elapsed: 155.6s | Est. remaining: 14.1s\n",
      "    âœ“ Generated explanation for ZB000901 - Timeless Savings Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000901 - Aspire Account (Rank 2)\n",
      "    âœ“ Generated explanation for ZB000901 - Timeless Current Account (Rank 1)\n",
      "    âœ“ Generated explanation for ZB000991 - Aspire Account (Rank 3)\n",
      "    âœ“ Generated explanation for ZB000991 - Timeless Savings Account (Rank 2)\n",
      "  â†’ Progress: 60/60 (100%) | Elapsed: 170.1s | Est. remaining: 0.0s\n",
      "    âœ“ Generated explanation for ZB000991 - Timeless Current Account (Rank 1)\n",
      "\n",
      "  âœ… Completed 60 explanations in 172.4s (avg 2.87s per explanation)\n",
      "\n",
      "ðŸ’¾ Saving final recommendations table...\n",
      "âœ… Table 2 saved: final_recommendations_api_table\n",
      "\n",
      "ðŸ“Š SAMPLE RESULTS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "+-----------+------------------------+---------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|Customer_ID|Product_Name            |Confidence_Score_Percentage|Recommendation_Reason                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |Rank|\n",
      "+-----------+------------------------+---------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|ZB000071   |Timeless Current Account|48.35237                   |This customer's profile as a 29-year-old lawyer with an income exceeding â‚¦1,000,000 positions him as an ideal candidate for the Timeless Current Account. The product's features are well-suited to his professional and financial status, offering benefits that cater to high-income professionals who may require premium banking services. The recommendation as the top choice underscores the alignment of the product's offerings with the client's potential need for efficient financial management solutions.|1   |\n",
      "|ZB000071   |Aspire Account          |47.30401                   |The client's profile as a 29-year-old Lawyer with an income exceeding â‚¦1,000,000 aligns well with the Aspire Account, which is designed to meet the financial needs of high-earning professionals. Given their age and income bracket, this account can provide tailored benefits and flexibility suited for their career stage and financial aspirations.                                                                                                                                                             |2   |\n",
      "|ZB000071   |Timeless Savings Account|45.10952                   |This customer's profile as a 29-year-old male lawyer with an income exceeding â‚¦1,000,000 suggests a potential interest in long-term financial growth and stability, making the Timeless Savings Account a suitable option. While it ranks as the third-best recommendation, the account's benefits, such as competitive interest rates and flexibility, align with their professional and financial aspirations, providing a solid foundation for future financial planning.                                           |3   |\n",
      "|ZB000101   |Timeless Current Account|51.09323                   |This customer's profile as a 60-year-old HR Manager in the â‚¦500,000 - â‚¦1,000,000 income bracket makes them an ideal candidate for the Timeless Current Account. Their age suggests they would benefit from a product offering stability and tailored financial solutions, while their income level aligns with the account's features designed for individuals seeking efficient financial management.                                                                                                                 |1   |\n",
      "|ZB000101   |Aspire Account          |48.854652                  |This customerâ€™s profile as an HR Manager within the â‚¦500,000 - â‚¦1,000,000 income bracket at age 60 makes the Aspire Account a suitable recommendation. Their professional background and financial standing suggest they could benefit from the account's tailored offerings, such as competitive interest rates and personalized financial services, which are designed to support their lifestyle and future financial planning needs.                                                                               |2   |\n",
      "|ZB000101   |Timeless Savings Account|47.85328                   |The client's age of 60 and income bracket of â‚¦500,000 - â‚¦1,000,000 suggest that the Timeless Savings Account is a suitable option, providing a secure and stable means to grow their savings as they approach retirement. As an HR Manager, the customer likely values financial products that offer reliability and potentially favorable interest rates, making this a fitting choice despite being the third-ranked recommendation.                                                                                 |3   |\n",
      "|ZB000121   |Aspire Account          |52.815613                  |This customer, being a 28-year-old student with an income bracket of â‚¦0,100,000, makes an ideal candidate for the Aspire Account. The product is tailored to meet the financial needs of students, offering features that accommodate their likely fluctuating income and focus on financial growth. The recommendation is ranked highest due to its alignment with the client's demographic and educational profile, providing them with relevant benefits and manageable terms.                                      |1   |\n",
      "|ZB000121   |Timeless Current Account|52.107624                  |This customer's profile as a 28-year-old student with an income bracket of â‚¦0,100,000 makes them an ideal candidate for the Timeless Current Account. This account offers flexibility and features tailored for individuals transitioning into financial independence, aligning well with the client's current life stage and financial situation.                                                                                                                                                                     |2   |\n",
      "|ZB000121   |Timeless Savings Account|48.762268                  |This customer's profile as a 28-year-old Student in the â‚¦0,100,000 income bracket suggests that the Timeless Savings Account could be a suitable option, though it ranks as the third-best choice. Their current life stage and financial status may align with the account's features, offering a straightforward way to save and potentially earn interest, despite the moderate model confidence.                                                                                                                   |3   |\n",
      "|ZB000161   |Timeless Current Account|59.25241                   |The client's profile as a 51-year-old accountant with an income between â‚¦100,000 and â‚¦500,000 makes her an ideal candidate for the Timeless Current Account. Her financial expertise and maturity suggest she would appreciate the account's tailored features, such as competitive interest rates and flexible access, which align well with her income stability and potential need for reliable transaction services.                                                                                               |1   |\n",
      "+-----------+------------------------+---------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "====================================================================================================\n",
      "âœ… RECOMMENDATION SYSTEM COMPLETE!\n",
      "====================================================================================================\n",
      "  ðŸ“Š ALS Model Performance:\n",
      "     â€¢ RMSE: 0.2467\n",
      "     â€¢ Rank: 20\n",
      "     â€¢ Regularization: 0.1\n",
      "\n",
      "  ðŸ‘¥ Coverage:\n",
      "     â€¢ Total Customers: 999\n",
      "     â€¢ Total Products: 29\n",
      "\n",
      "  ðŸ’¾ Output Tables:\n",
      "     â€¢ ALS recommendations: 4,995 rows â†’ als_recommendations_table\n",
      "     â€¢ Final recommendations with LLM: 60 rows â†’ final_recommendations_api_table\n",
      "\n",
      "  â±ï¸  LLM Generation:\n",
      "     â€¢ Total time: 172.4s\n",
      "     â€¢ Avg per recommendation: 2.87s\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================  \n",
    "# UNITY CATALOG COMPATIBLE ALS + LLM RECOMMENDATION SYSTEM (FIXED)\n",
    "# ============================================================================  \n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, row_number, desc, lit\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField, IntegerType\n",
    "from openai import OpenAI\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================  \n",
    "# CONFIGURATION  \n",
    "# ============================================================================  \n",
    "\n",
    "OPENAI_API_KEY = \"**REDACTED**\"\n",
    "OPENAI_MODEL = \"gpt-4o\"\n",
    "\n",
    "TOP_N = 5\n",
    "ALS_RANK = 20\n",
    "ALS_REG_PARAM = 0.1\n",
    "ALS_MAX_ITER = 20\n",
    "\n",
    "INTERACTIONS_DF = interaction_df  # Your preloaded interactions\n",
    "CUSTOMERS_DF = customer_features    # Your preloaded customer features\n",
    "PRODUCTS_DF = product_map\n",
    "\n",
    "# For testing - limit to N customers\n",
    "SAMPLE_CUSTOMERS = 20\n",
    "\n",
    "# ============================================================================  \n",
    "# STEP 1: CLEAN DATA  \n",
    "# ============================================================================  \n",
    "\n",
    "print(\"\\nðŸ“‹ Step 1: Preparing data...\")\n",
    "\n",
    "interaction_df = INTERACTIONS_DF\n",
    "\n",
    "# Validate columns\n",
    "required_cols = {\"Customer_ID\", \"Product_Name\", \"interaction_score\"}\n",
    "missing = required_cols - set(interaction_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "# Clean\n",
    "interaction_df = (interaction_df\n",
    "    .filter(col(\"Customer_ID\").isNotNull())\n",
    "    .filter(col(\"Product_Name\").isNotNull())\n",
    "    .filter(col(\"interaction_score\").isNotNull())\n",
    "    .withColumn(\"interaction_score\", col(\"interaction_score\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "print(f\"âœ… Cleaned: {interaction_df.count():,} interactions\")\n",
    "\n",
    "# Deduplicate per Customer-Product\n",
    "w_dedup = Window.partitionBy(\"Customer_ID\", \"Product_Name\").orderBy(desc(\"interaction_score\"))\n",
    "interaction_df = (interaction_df\n",
    "    .withColumn(\"rn\", row_number().over(w_dedup))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "customer_features = CUSTOMERS_DF\n",
    "\n",
    "# ============================================================================  \n",
    "# STEP 2: CREATE INDEXES  \n",
    "# ============================================================================  \n",
    "\n",
    "print(\"\\nðŸ“‹ Step 2: Creating indexes...\")\n",
    "\n",
    "customer_lookup = (interaction_df\n",
    "    .select(\"Customer_ID\")\n",
    "    .distinct()\n",
    "    .withColumn(\"user_int\", (row_number().over(Window.orderBy(\"Customer_ID\")) - 1).cast(\"int\"))\n",
    ")\n",
    "\n",
    "product_lookup = (interaction_df\n",
    "    .select(\"Product_Name\")\n",
    "    .distinct()\n",
    "    .withColumn(\"item_int\", (row_number().over(Window.orderBy(\"Product_Name\")) - 1).cast(\"int\"))\n",
    ")\n",
    "\n",
    "print(f\"  âœ“ Customers: {customer_lookup.count():,}\")\n",
    "print(f\"  âœ“ Products: {product_lookup.count():,}\")\n",
    "\n",
    "# Indexed interactions\n",
    "idx_df = (interaction_df\n",
    "    .join(customer_lookup, \"Customer_ID\", \"inner\")\n",
    "    .join(product_lookup, \"Product_Name\", \"inner\")\n",
    "    .select(\"Customer_ID\", \"Product_Name\", \"interaction_score\", \"user_int\", \"item_int\")\n",
    ")\n",
    "\n",
    "# ============================================================================  \n",
    "# STEP 3: TRAIN ALS  \n",
    "# ============================================================================  \n",
    "\n",
    "print(\"\\nðŸ”¨ Step 3: Training ALS...\")\n",
    "\n",
    "train_df, test_df = idx_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"user_int\",\n",
    "    itemCol=\"item_int\",\n",
    "    ratingCol=\"interaction_score\",\n",
    "    implicitPrefs=False,\n",
    "    nonnegative=True,\n",
    "    coldStartStrategy=\"drop\",\n",
    "    rank=ALS_RANK,\n",
    "    regParam=ALS_REG_PARAM,\n",
    "    maxIter=ALS_MAX_ITER,\n",
    "    seed=42\n",
    ")\n",
    "als_model = als.fit(train_df)\n",
    "\n",
    "pred_test = als_model.transform(test_df)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"interaction_score\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "rmse = evaluator.evaluate(pred_test)\n",
    "print(f\"âœ… Trained. RMSE: {rmse:.4f}\")\n",
    "\n",
    "# ============================================================================  \n",
    "# STEP 4: GENERATE RECOMMENDATIONS (UNITY CATALOG COMPATIBLE)\n",
    "# ============================================================================  \n",
    "\n",
    "print(\"\\nðŸ“‹ Step 4: Generating recommendations...\")\n",
    "\n",
    "# Extract factor matrices (Unity Catalog safe)\n",
    "user_factors = als_model.userFactors.collect()\n",
    "item_factors = als_model.itemFactors.collect()\n",
    "\n",
    "# Build lookup dictionaries\n",
    "user_factor_dict = {row['id']: row['features'] for row in user_factors}\n",
    "item_factor_dict = {row['id']: row['features'] for row in item_factors}\n",
    "\n",
    "print(f\"  âœ“ Users: {len(user_factor_dict):,}, Items: {len(item_factor_dict):,}\")\n",
    "\n",
    "# Compute recommendations manually in Python (bypasses Unity Catalog restrictions)\n",
    "recs_list = []\n",
    "for user_int, user_vec in user_factor_dict.items():\n",
    "    user_array = np.array(user_vec)\n",
    "    scores = []\n",
    "    \n",
    "    for item_int, item_vec in item_factor_dict.items():\n",
    "        item_array = np.array(item_vec)\n",
    "        score = np.dot(user_array, item_array)\n",
    "        scores.append((item_int, float(score)))\n",
    "    \n",
    "    # Sort by score and get top N\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_items = scores[:TOP_N]\n",
    "    \n",
    "    for rank, (item_int, als_score) in enumerate(top_items, start=1):\n",
    "        recs_list.append({\n",
    "            'user_int': int(user_int),\n",
    "            'item_int': int(item_int),\n",
    "            'als_score': float(als_score),\n",
    "            'rank': rank\n",
    "        })\n",
    "\n",
    "print(f\"  âœ“ Computed {len(recs_list):,} recommendations\")\n",
    "\n",
    "# Create DataFrame from collected data\n",
    "rec_schema = StructType([\n",
    "    StructField(\"user_int\", IntegerType(), True),\n",
    "    StructField(\"item_int\", IntegerType(), True),\n",
    "    StructField(\"als_score\", FloatType(), True),\n",
    "    StructField(\"rank\", IntegerType(), True)\n",
    "])\n",
    "als_flat = spark.createDataFrame(recs_list, schema=rec_schema)\n",
    "\n",
    "# Join lookups\n",
    "als_recommendations = (als_flat\n",
    "    .join(customer_lookup, \"user_int\", \"left\")\n",
    "    .join(product_lookup, \"item_int\", \"left\")\n",
    ")\n",
    "\n",
    "# Serverless-safe confidence score (fixed min/max)\n",
    "min_score = 0.0\n",
    "max_score = 5.0  # Expected ALS score range\n",
    "als_recommendations = als_recommendations.withColumn(\n",
    "    \"confidence_score_pct\",\n",
    "    ((col(\"als_score\") - lit(min_score)) / lit(max_score - min_score) * 100)\n",
    ")\n",
    "\n",
    "# ============================================================================  \n",
    "# SAVE TABLE 1: ALS RECOMMENDATIONS  \n",
    "# ============================================================================  \n",
    "\n",
    "print(\"\\nðŸ’¾ Saving ALS table...\")\n",
    "\n",
    "# Collect and recreate to break lineage completely\n",
    "als_data = als_recommendations.select(\n",
    "    \"Customer_ID\", \"Product_Name\", \"als_score\", \"confidence_score_pct\", \"rank\"\n",
    ").collect()\n",
    "\n",
    "als_output_schema = StructType([\n",
    "    StructField(\"Customer_ID\", StringType(), True),\n",
    "    StructField(\"Product_Name\", StringType(), True),\n",
    "    StructField(\"als_score\", FloatType(), True),\n",
    "    StructField(\"confidence_score_pct\", FloatType(), True),\n",
    "    StructField(\"rank\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "als_output_data = [{\n",
    "    'Customer_ID': row['Customer_ID'],\n",
    "    'Product_Name': row['Product_Name'],\n",
    "    'als_score': float(row['als_score']),\n",
    "    'confidence_score_pct': float(row['confidence_score_pct']),\n",
    "    'rank': int(row['rank'])\n",
    "} for row in als_data]\n",
    "\n",
    "als_output = spark.createDataFrame(als_output_data, schema=als_output_schema).orderBy(\"Customer_ID\", \"rank\")\n",
    "\n",
    "als_output.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"als_recommendations_table\")\n",
    "print(\"âœ… Table 1 saved: als_recommendations_table\")\n",
    "\n",
    "# ============================================================================  \n",
    "# STEP 5: GENERATE LLM EXPLANATIONS (IMPROVED WITH ERROR HANDLING)\n",
    "# ============================================================================  \n",
    "\n",
    "print(f\"\\nðŸ¤– Step 5: Generating LLM explanations (limiting to {SAMPLE_CUSTOMERS} customers)...\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def generate_llm_explanation(customer_id, product_name, confidence_pct, rank, cust_dict):\n",
    "    \"\"\"Generate natural language recommendation explanation for account managers\"\"\"\n",
    "    try:\n",
    "        # Extract customer features with proper defaults\n",
    "        age = cust_dict.get('Age', 'Unknown')\n",
    "        occupation = cust_dict.get('Occupation', 'Unknown')\n",
    "        income = cust_dict.get('Income_Bracket', 'Unknown')\n",
    "        account_type = cust_dict.get('Account_Type', 'Unknown')\n",
    "        gender = cust_dict.get('Gender', 'Unknown')\n",
    "        location = cust_dict.get('Location', 'Unknown')\n",
    "        \n",
    "        # Rank context for the account manager\n",
    "        if rank == 1:\n",
    "            rank_context = \"This is the TOP recommendation for this customer.\"\n",
    "        elif rank == 2:\n",
    "            rank_context = \"This is the SECOND-BEST option for this customer.\"\n",
    "        elif rank == 3:\n",
    "            rank_context = \"This is the THIRD-BEST option for this customer.\"\n",
    "        else:\n",
    "            rank_context = f\"This ranks #{rank} for this customer.\"\n",
    "        \n",
    "        # Build detailed prompt for account manager\n",
    "        prompt = f\"\"\"You are an AI assistant helping bank account managers make product recommendations. Provide a brief explanation (2-3 sentences) for why this product suits this customer's profile.\n",
    "\n",
    "CUSTOMER PROFILE (ID: {customer_id}):\n",
    "- Age: {age}\n",
    "- Gender: {gender}\n",
    "- Occupation: {occupation}\n",
    "- Income Bracket: {income}\n",
    "- Current Account: {account_type}\n",
    "- Location: {location}\n",
    "\n",
    "RECOMMENDED PRODUCT: {product_name}\n",
    "RECOMMENDATION RANK: #{rank} (out of top 3)\n",
    "MODEL CONFIDENCE: {confidence_pct:.1f}%\n",
    "\n",
    "{rank_context}\n",
    "\n",
    "Write a professional explanation for the account manager that:\n",
    "1. Uses third-person language (e.g., \"This customer...\", \"The client's...\", \"Their profile indicates...\")\n",
    "2. References specific customer attributes that justify this recommendation\n",
    "3. Explains the product fit based on their demographics and financial profile\n",
    "4. Is concise and actionable (2-3 sentences max)\n",
    "5. Considers the ranking - top recommendations should emphasize strongest fit factors\n",
    "\n",
    "Example style: \"This customer's profile as a {occupation} in the {income} bracket makes them an ideal candidate for {product_name}. Their {age} and {account_type} indicate they would benefit from...\"\n",
    "\n",
    "DO NOT use second-person language like \"you\" or \"your\".\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a professional banking analytics assistant providing product recommendation rationale to account managers. Always use third-person language.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        explanation = response.choices[0].message.content.strip()\n",
    "        print(f\"    âœ“ Generated explanation for {customer_id} - {product_name} (Rank {rank})\")\n",
    "        return explanation\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"    âœ— Error for {customer_id}: {error_msg[:100]}\")\n",
    "        # Professional fallback for account managers\n",
    "        return f\"This customer's profile and banking behavior patterns indicate strong alignment with {product_name}. Recommendation confidence: {confidence_pct:.1f}%.\"\n",
    "\n",
    "# Get top N customers and their top 3 recommendations\n",
    "print(f\"  â†’ Selecting top {SAMPLE_CUSTOMERS} customers...\")\n",
    "top_customers = (als_recommendations\n",
    "    .select(\"Customer_ID\")\n",
    "    .distinct()\n",
    "    .limit(SAMPLE_CUSTOMERS)\n",
    "    .collect()\n",
    ")\n",
    "top_customer_ids = [row['Customer_ID'] for row in top_customers]\n",
    "\n",
    "# Filter to only these customers, top 3 each\n",
    "top_3_recs = (als_recommendations\n",
    "    .filter(col(\"Customer_ID\").isin(top_customer_ids))\n",
    "    .filter(col(\"rank\") <= 3)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(f\"  âœ“ Processing {len(top_customer_ids)} customers, {len(top_3_recs)} recommendations...\")\n",
    "\n",
    "# Pre-fetch ALL customer features efficiently\n",
    "print(\"  â†’ Fetching customer features...\")\n",
    "customer_features_list = customer_features.filter(\n",
    "    col(\"Customer_ID\").isin(top_customer_ids)\n",
    ").collect()\n",
    "\n",
    "customer_dict = {row['Customer_ID']: row.asDict() for row in customer_features_list}\n",
    "print(f\"  âœ“ Loaded {len(customer_dict)} customer profiles\")\n",
    "\n",
    "# Generate explanations with progress tracking\n",
    "llm_results = []\n",
    "total_recs = len(top_3_recs)\n",
    "start_time = datetime.now()\n",
    "\n",
    "print(f\"\\n  ðŸ”„ Starting LLM generation at {start_time.strftime('%H:%M:%S')}...\")\n",
    "\n",
    "for idx, row in enumerate(top_3_recs, 1):\n",
    "    if idx % 5 == 0 or idx == 1:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds()\n",
    "        avg_time = elapsed / idx if idx > 0 else 0\n",
    "        remaining = avg_time * (total_recs - idx)\n",
    "        print(f\"  â†’ Progress: {idx}/{total_recs} ({idx*100//total_recs}%) | Elapsed: {elapsed:.1f}s | Est. remaining: {remaining:.1f}s\")\n",
    "    \n",
    "    cust_dict = customer_dict.get(row['Customer_ID'], {})\n",
    "    \n",
    "    # Generate explanation\n",
    "    llm_reason = generate_llm_explanation(\n",
    "        row['Customer_ID'], \n",
    "        row['Product_Name'], \n",
    "        row['confidence_score_pct'],\n",
    "        row['rank'],  # Pass rank to function\n",
    "        cust_dict\n",
    "    )\n",
    "    \n",
    "    llm_results.append({\n",
    "        'Customer_ID': row['Customer_ID'],\n",
    "        'Product_Name': row['Product_Name'],\n",
    "        'ALS_Score': float(row['als_score']),\n",
    "        'Confidence_Score_PCT': float(row['confidence_score_pct']),\n",
    "        'Rank': int(row['rank']),\n",
    "        'LLM_Reason': llm_reason\n",
    "    })\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_time = (end_time - start_time).total_seconds()\n",
    "print(f\"\\n  âœ… Completed {len(llm_results)} explanations in {total_time:.1f}s (avg {total_time/len(llm_results):.2f}s per explanation)\")\n",
    "\n",
    "# Create final Spark DataFrame\n",
    "llm_schema = StructType([\n",
    "    StructField(\"Customer_ID\", StringType(), True),\n",
    "    StructField(\"Product_Name\", StringType(), True),\n",
    "    StructField(\"ALS_Score\", FloatType(), True),\n",
    "    StructField(\"Confidence_Score_PCT\", FloatType(), True),\n",
    "    StructField(\"Rank\", IntegerType(), True),\n",
    "    StructField(\"LLM_Reason\", StringType(), True)\n",
    "])\n",
    "final_recommendations = spark.createDataFrame(llm_results, schema=llm_schema)\n",
    "\n",
    "# ============================================================================  \n",
    "# SAVE TABLE 2: FINAL RECOMMENDATIONS  \n",
    "# ============================================================================  \n",
    "\n",
    "print(\"\\nðŸ’¾ Saving final recommendations table...\")\n",
    "\n",
    "final_output = (final_recommendations\n",
    "    .select(\n",
    "        \"Customer_ID\",\n",
    "        \"Product_Name\",\n",
    "        col(\"Confidence_Score_PCT\").alias(\"Confidence_Score_Percentage\"),\n",
    "        col(\"LLM_Reason\").alias(\"Recommendation_Reason\"),\n",
    "        \"Rank\"\n",
    "    )\n",
    "    .orderBy(\"Customer_ID\", \"Rank\")\n",
    ")\n",
    "final_output.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"final_recommendations_api_table\")\n",
    "print(\"âœ… Table 2 saved: final_recommendations_api_table\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nðŸ“Š SAMPLE RESULTS:\")\n",
    "print(\"-\" * 100)\n",
    "final_output.show(10, truncate=False)\n",
    "\n",
    "# ============================================================================  \n",
    "# SUMMARY  \n",
    "# ============================================================================  \n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"âœ… RECOMMENDATION SYSTEM COMPLETE!\")\n",
    "print(\"=\"*100)\n",
    "print(f\"  ðŸ“Š ALS Model Performance:\")\n",
    "print(f\"     â€¢ RMSE: {rmse:.4f}\")\n",
    "print(f\"     â€¢ Rank: {ALS_RANK}\")\n",
    "print(f\"     â€¢ Regularization: {ALS_REG_PARAM}\")\n",
    "print(f\"\\n  ðŸ‘¥ Coverage:\")\n",
    "print(f\"     â€¢ Total Customers: {len(user_factor_dict):,}\")\n",
    "print(f\"     â€¢ Total Products: {len(item_factor_dict):,}\")\n",
    "print(f\"\\n  ðŸ’¾ Output Tables:\")\n",
    "print(f\"     â€¢ ALS recommendations: {len(als_output_data):,} rows â†’ als_recommendations_table\")\n",
    "print(f\"     â€¢ Final recommendations with LLM: {len(llm_results):,} rows â†’ final_recommendations_api_table\")\n",
    "print(f\"\\n  â±ï¸  LLM Generation:\")\n",
    "print(f\"     â€¢ Total time: {total_time:.1f}s\")\n",
    "print(f\"     â€¢ Avg per recommendation: {total_time/len(llm_results):.2f}s\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250ebcc4-c221-43b2-aa74-a9603180059d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------------+\n",
      "|Customer_ID|        Product_Name| interaction_score|\n",
      "+-----------+--------------------+------------------+\n",
      "|   ZB000001|       Personal Loan|2.1000000000000005|\n",
      "|   ZB000001|Zenith Individual...|               2.7|\n",
      "|   ZB000001|Zenith Internet B...|2.4000000000000004|\n",
      "|   ZB000001|Zenith Mobile Ban...|               2.7|\n",
      "|   ZB000001|Zenith Salary Sav...|2.4000000000000004|\n",
      "|   ZB000002|SME Grow My Biz A...|2.4000000000000004|\n",
      "|   ZB000002|            SME Loan|2.1000000000000005|\n",
      "|   ZB000002|Zenith Individual...|2.1000000000000005|\n",
      "|   ZB000002|Zenith Internet B...|2.4000000000000004|\n",
      "|   ZB000002|Zenith Mobile Ban...|               2.7|\n",
      "|   ZB000003|       Personal Loan|1.8000000000000003|\n",
      "|   ZB000003|Zenith Individual...|2.4000000000000004|\n",
      "|   ZB000003|Zenith Internet B...|2.4000000000000004|\n",
      "|   ZB000003|Zenith Mobile Ban...|               2.7|\n",
      "|   ZB000003| Zenith Virtual Card|2.1000000000000005|\n",
      "|   ZB000004|       Personal Loan|2.1000000000000005|\n",
      "|   ZB000004|Zenith Internet B...|2.4000000000000004|\n",
      "|   ZB000004|Zenith Mobile Ban...|               2.7|\n",
      "|   ZB000005|       Personal Loan|2.1000000000000005|\n",
      "|   ZB000005|Zenith Individual...|               2.7|\n",
      "+-----------+--------------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "interaction_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "zenith",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
